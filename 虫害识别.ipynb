{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "import os\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多\n",
    "#DEVICE = torch.device(\"cpu\") \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')#有时候运行代码时会有很多warning输出，如提醒新版本之类的，如果不想这些乱糟糟的输出可以这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)#加载预训练的模型，也可以不加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)#查看renet50的网络结构，如果不修改网络结构来训练自己的模型的话，只需要修改全连接层的输入和输出即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fc = nn.Linear(2048,33)#只修改最后一层全连接模型，由于我们昆虫数据集只有33个分类\n",
    "\n",
    "\n",
    "#或者这种写法\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=2048, out_features=17, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)#查看模型是否被成果修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把模型指定在GPU上运行\n",
    "#model = model.to(DEVICE)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data0/zengpz'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读取数据，并对数据进行归一化处理\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "picturepath = '/data0/zengpz/damage'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "damage\n",
      "/data0/zengpz\n",
      "['/data0/zengpz/damage/黑斑病', '/data0/zengpz/damage/叶斑病', '/data0/zengpz/damage/蛴螬', '/data0/zengpz/damage/软腐病', '/data0/zengpz/damage/炭疽病', '/data0/zengpz/damage/煤污病', '/data0/zengpz/damage/叶枯病', '/data0/zengpz/damage/流胶病', '/data0/zengpz/damage/蛾', '/data0/zengpz/damage/褐斑病', '/data0/zengpz/damage/介壳虫', '/data0/zengpz/damage/白粉病', '/data0/zengpz/damage/螨', '/data0/zengpz/damage/根腐病', '/data0/zengpz/damage/蚜', '/data0/zengpz/damage/飞虱', '/data0/zengpz/damage/马陆']\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "print(os.path.basename(picturepath))\n",
    "print(os.path.dirname(picturepath))\n",
    "\n",
    "import glob\n",
    "\n",
    "print(glob.glob('/data0/zengpz/damage/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "['黑斑病', '叶斑病', '蛴螬', '软腐病', '炭疽病', '煤污病', '叶枯病', '流胶病', '蛾', '褐斑病', '介壳虫', '白粉病', '螨', '根腐病', '蚜', '飞虱', '马陆']\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "list_dir = [i for i in glob.glob('/data0/zengpz/damage/*')]\n",
    "print(len(list_dir))\n",
    "#show其中一张图片的路径\n",
    "list_dir_ =[]\n",
    "for i in range(len(list_dir)):\n",
    "    list_dir_ .append(list_dir[i].split('/')[4])\n",
    "print(list_dir_)\n",
    "print(len(list_dir_))#可以看到一共17个类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "#files = sorted(glob.glob('/data0/zengpz/insect（我处理）/桑褐刺蛾/*.jpg'))\n",
    "\n",
    "files = sorted(glob.glob('/data0/zengpz/damage/*/*.jpg'))\n",
    "\n",
    "#print(files[32])\n",
    "print(len(files))#一共3400张照片\n",
    "#print(files)\n",
    "#t = '/data0/zengpz/insect（我处理）/桑褐刺蛾/1.jpg'\n",
    "#t in files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3162\n",
      "3162\n",
      "                       chonghai_dir chonghai_label\n",
      "0    /data0/zengpz/damage/介壳虫/1.jpg            介壳虫\n",
      "1   /data0/zengpz/damage/介壳虫/10.jpg            介壳虫\n",
      "2  /data0/zengpz/damage/介壳虫/100.jpg            介壳虫\n",
      "3  /data0/zengpz/damage/介壳虫/101.jpg            介壳虫\n",
      "4  /data0/zengpz/damage/介壳虫/102.jpg            介壳虫\n"
     ]
    }
   ],
   "source": [
    "#查看显示某一张图片\n",
    "from PIL import Image\n",
    "files = sorted(glob.glob('/data0/zengpz/damage/*/*.jpg'))\n",
    "#im = Image.open(r'list_dir[0]/')\n",
    "#im.show()\n",
    "image_labels = []\n",
    "filespath =[]\n",
    "for file in files:  \n",
    "    image_labels.append(file.split('/')[4])\n",
    "    filespath.append(file)\n",
    "    \n",
    "print(len(filespath))#一共3400张照片\n",
    "print(len(image_labels))\n",
    "\n",
    "#把路径和label存成dataframe的形式\n",
    "from pandas import Series,DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = DataFrame()\n",
    "df['chonghai_dir'] = filespath\n",
    "df['chonghai_label'] = image_labels\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "#保存成csv文件，便于之后使用\n",
    "#\n",
    "#df.to_csv('damage_dataread.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "#把数据随机按照比例1:10划分训练集和测试集\n",
    "test_index = range(0,3162,10) \n",
    "train_index = []\n",
    "\n",
    "for i in range(0,3162):\n",
    "    if i not in test_index:\n",
    "        train_index.append(i)\n",
    "\n",
    "train = df.iloc[train_index,:]\n",
    "test = df.iloc[test_index,:]\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "#保存train数据\n",
    "#train.to_csv('damage_train.csv')\n",
    "#test.to_csv('damage_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#显示某张图片看一下\n",
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(df['chonghai_dir'][3])\n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对原始图片的预处理\n",
    "from torchvision import transforms as transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "    transforms.RandomRotation((-45,45)), #随机旋转\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2845\n",
      "317\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "#定义一个数据集\n",
    "class myDataset(Dataset):\n",
    "    \"\"\" 数据集演示 \"\"\"\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"实现初始化方法，在初始化的时候将数据读载入\"\"\"\n",
    "        self.df=pd.read_csv(csv_file)\n",
    "        self.tags = {'黑斑病':0, '叶斑病':1, '蛴螬':2, '软腐病':3, '炭疽病':4, '煤污病':5,\n",
    "                      '叶枯病':6, '流胶病':7, '蛾':8, '褐斑病':9, '介壳虫':10, '白粉病':11,\n",
    "                      '螨':12, '根腐病':13, '蚜':14, '飞虱':15, '马陆':16}\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回df的长度\n",
    "        '''\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        根据 idx 返回一行数据\n",
    "        \n",
    "        '''\n",
    "        img = Image.open(self.df.iloc[idx].chonghai_dir).convert('RGB')\n",
    "        label = self.df.iloc[idx].chonghai_label\n",
    "        label = self.tags[label]\n",
    "        img_path = self.df.iloc[idx].chonghai_dir\n",
    "        #把图片做归一化处理\n",
    "        img = transform(img)\n",
    "        return img,label,img_path\n",
    "\n",
    "#修改读取的文件的路径    \n",
    "#ds_demo= myDataset('insect_dataread.csv')\n",
    "train_data =  myDataset('damage_train.csv')\n",
    "test_data = myDataset('damage_test.csv')\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 10,\n",
       " '/data0/zengpz/damage/介壳虫/100.jpg')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]#用索引可以直接访问对应的数据, 对应 __getitem__ 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][0].shape  #可以看到输入的图片已经做了归一化处理了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlist_ = []\\nfor i in range(0,2845):\\n    list_.append(train_data[i][1])\\n    if train_data[i][1] < 0 or train_data[i][1] > 17:\\n        print(train_data[i][2])\\nprint(len(set(list_)))\\nset(list_)\\n'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试一下是否真的有17类数据来训练\n",
    "'''\n",
    "list_ = []\n",
    "for i in range(0,2845):\n",
    "    list_.append(train_data[i][1])\n",
    "    if train_data[i][1] < 0 or train_data[i][1] > 17:\n",
    "        print(train_data[i][2])\n",
    "print(len(set(list_)))\n",
    "set(list_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#使用官方提供的数据载入器，读取数据\n",
    "#常用参数有：batch_size(每个batch的大小), shuffle(是否进行shuffle操作), num_workers(加载数据的时候使用几个子进程)\n",
    "train_ = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "test_ = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "#DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据\n",
    "#idata=iter(train)\n",
    "#print(next(idata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loss function and optimizer\n",
    "criterion=torch.nn.CrossEntropyLoss()\n",
    "#parameters only train the last fc layer\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def trainmodel(model,train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (inputs_, labels_, _) in enumerate(train_loader):\n",
    "    #for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #inputs = inputs.to(device)#用GPU训练\n",
    "        #labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        cuda = torch.device('cuda') \n",
    "        #直接用torch.tensor会有warning\n",
    "        inputs = torch.tensor(inputs_).cuda()\n",
    "        labels = torch.tensor(labels_).cuda()\n",
    "        #inputs = inputs_.clone().detach().requires_grad_(True)\n",
    "        #labels = labels_.clone().detach()\n",
    "        \n",
    "        \n",
    "        #inputs = inputs.to(device)#用GPU训练\n",
    "        #labels = labels.to(device)\n",
    "        #inputs = inputs.cpu()\n",
    "        #labels = labels.cpu()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        \n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(inputs), len(train_loader)*len(inputs),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "accuracy = []\n",
    "def testmodel(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels, _) in enumerate(test_loader):\n",
    "            \n",
    "            cuda = torch.device('cuda') \n",
    "            #这样有warning\n",
    "            data = torch.tensor(inputs).cuda()\n",
    "            target = torch.tensor(labels).cuda()\n",
    "            #data = inputs.clone().detach().requires_grad_(True)\n",
    "            #target = labels.clone().detach\n",
    "            \n",
    "            \n",
    "            #device = torch.device(\"cuda\")\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #data, target = data.cpu(), target.cpu()\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    accuracy.append(100. * correct / (len(inputs)*len(test_loader)))\n",
    "    if 100. * correct / (len(inputs)*len(test_loader)) >= max(accuracy):\n",
    "        torch.save(model.state_dict,'resnet50_chonghai_parameters.pkl')\n",
    "    test_loss /= len(inputs)*len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(inputs)*len(test_loader),\n",
    "        100. * correct / (len(inputs)*len(test_loader))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [232/2848 (8%)]\tLoss: 2.449261\n",
      "Train Epoch: 1 [472/2848 (17%)]\tLoss: 2.630345\n",
      "Train Epoch: 1 [712/2848 (25%)]\tLoss: 2.864708\n",
      "Train Epoch: 1 [952/2848 (33%)]\tLoss: 2.527807\n",
      "Train Epoch: 1 [1192/2848 (42%)]\tLoss: 2.587501\n",
      "Train Epoch: 1 [1432/2848 (50%)]\tLoss: 2.872996\n",
      "Train Epoch: 1 [1672/2848 (59%)]\tLoss: 2.821906\n",
      "Train Epoch: 1 [1912/2848 (67%)]\tLoss: 2.780685\n",
      "Train Epoch: 1 [2152/2848 (76%)]\tLoss: 2.985184\n",
      "Train Epoch: 1 [2392/2848 (84%)]\tLoss: 2.985437\n",
      "Train Epoch: 1 [2632/2848 (92%)]\tLoss: 2.475358\n",
      "\n",
      "Test set: Average loss: 0.5219, Accuracy: 33/200 (16%)\n",
      "\n",
      "Train Epoch: 2 [232/2848 (8%)]\tLoss: 2.737757\n",
      "Train Epoch: 2 [472/2848 (17%)]\tLoss: 2.811751\n",
      "Train Epoch: 2 [712/2848 (25%)]\tLoss: 2.810771\n",
      "Train Epoch: 2 [952/2848 (33%)]\tLoss: 2.610869\n",
      "Train Epoch: 2 [1192/2848 (42%)]\tLoss: 2.291857\n",
      "Train Epoch: 2 [1432/2848 (50%)]\tLoss: 2.555051\n",
      "Train Epoch: 2 [1672/2848 (59%)]\tLoss: 2.310217\n",
      "Train Epoch: 2 [1912/2848 (67%)]\tLoss: 2.640107\n",
      "Train Epoch: 2 [2152/2848 (76%)]\tLoss: 2.596848\n",
      "Train Epoch: 2 [2392/2848 (84%)]\tLoss: 2.525055\n",
      "Train Epoch: 2 [2632/2848 (92%)]\tLoss: 2.703444\n",
      "\n",
      "Test set: Average loss: 0.5184, Accuracy: 51/200 (26%)\n",
      "\n",
      "Train Epoch: 3 [232/2848 (8%)]\tLoss: 2.542104\n",
      "Train Epoch: 3 [472/2848 (17%)]\tLoss: 2.698581\n",
      "Train Epoch: 3 [712/2848 (25%)]\tLoss: 2.650589\n",
      "Train Epoch: 3 [952/2848 (33%)]\tLoss: 2.570203\n",
      "Train Epoch: 3 [1192/2848 (42%)]\tLoss: 2.545366\n",
      "Train Epoch: 3 [1432/2848 (50%)]\tLoss: 2.404890\n",
      "Train Epoch: 3 [1672/2848 (59%)]\tLoss: 2.755915\n",
      "Train Epoch: 3 [1912/2848 (67%)]\tLoss: 2.829386\n",
      "Train Epoch: 3 [2152/2848 (76%)]\tLoss: 2.607855\n",
      "Train Epoch: 3 [2392/2848 (84%)]\tLoss: 2.875869\n",
      "Train Epoch: 3 [2632/2848 (92%)]\tLoss: 3.046598\n",
      "\n",
      "Test set: Average loss: 0.5040, Accuracy: 57/200 (28%)\n",
      "\n",
      "Train Epoch: 4 [232/2848 (8%)]\tLoss: 2.583035\n",
      "Train Epoch: 4 [472/2848 (17%)]\tLoss: 2.650644\n",
      "Train Epoch: 4 [712/2848 (25%)]\tLoss: 2.382623\n",
      "Train Epoch: 4 [952/2848 (33%)]\tLoss: 2.894761\n",
      "Train Epoch: 4 [1192/2848 (42%)]\tLoss: 2.652033\n",
      "Train Epoch: 4 [1432/2848 (50%)]\tLoss: 2.637495\n",
      "Train Epoch: 4 [1672/2848 (59%)]\tLoss: 3.170883\n",
      "Train Epoch: 4 [1912/2848 (67%)]\tLoss: 2.528920\n",
      "Train Epoch: 4 [2152/2848 (76%)]\tLoss: 2.450758\n",
      "Train Epoch: 4 [2392/2848 (84%)]\tLoss: 2.626690\n",
      "Train Epoch: 4 [2632/2848 (92%)]\tLoss: 2.726977\n",
      "\n",
      "Test set: Average loss: 0.5064, Accuracy: 48/200 (24%)\n",
      "\n",
      "Train Epoch: 5 [232/2848 (8%)]\tLoss: 2.674946\n",
      "Train Epoch: 5 [472/2848 (17%)]\tLoss: 2.511848\n",
      "Train Epoch: 5 [712/2848 (25%)]\tLoss: 2.539183\n",
      "Train Epoch: 5 [952/2848 (33%)]\tLoss: 2.555368\n",
      "Train Epoch: 5 [1192/2848 (42%)]\tLoss: 2.271875\n",
      "Train Epoch: 5 [1432/2848 (50%)]\tLoss: 2.562648\n",
      "Train Epoch: 5 [1672/2848 (59%)]\tLoss: 2.512840\n",
      "Train Epoch: 5 [1912/2848 (67%)]\tLoss: 2.531551\n",
      "Train Epoch: 5 [2152/2848 (76%)]\tLoss: 2.356894\n",
      "Train Epoch: 5 [2392/2848 (84%)]\tLoss: 2.405555\n",
      "Train Epoch: 5 [2632/2848 (92%)]\tLoss: 2.767389\n",
      "\n",
      "Test set: Average loss: 0.4951, Accuracy: 50/200 (25%)\n",
      "\n",
      "Train Epoch: 6 [232/2848 (8%)]\tLoss: 2.395784\n",
      "Train Epoch: 6 [472/2848 (17%)]\tLoss: 2.721156\n",
      "Train Epoch: 6 [712/2848 (25%)]\tLoss: 2.609808\n",
      "Train Epoch: 6 [952/2848 (33%)]\tLoss: 2.376071\n",
      "Train Epoch: 6 [1192/2848 (42%)]\tLoss: 2.507788\n",
      "Train Epoch: 6 [1432/2848 (50%)]\tLoss: 2.765843\n",
      "Train Epoch: 6 [1672/2848 (59%)]\tLoss: 2.557170\n",
      "Train Epoch: 6 [1912/2848 (67%)]\tLoss: 2.658619\n",
      "Train Epoch: 6 [2152/2848 (76%)]\tLoss: 2.424604\n",
      "Train Epoch: 6 [2392/2848 (84%)]\tLoss: 2.845912\n",
      "Train Epoch: 6 [2632/2848 (92%)]\tLoss: 2.403223\n",
      "\n",
      "Test set: Average loss: 0.5241, Accuracy: 47/200 (24%)\n",
      "\n",
      "Train Epoch: 7 [232/2848 (8%)]\tLoss: 2.526577\n",
      "Train Epoch: 7 [472/2848 (17%)]\tLoss: 2.288713\n",
      "Train Epoch: 7 [712/2848 (25%)]\tLoss: 2.004531\n",
      "Train Epoch: 7 [952/2848 (33%)]\tLoss: 2.236844\n",
      "Train Epoch: 7 [1192/2848 (42%)]\tLoss: 2.569500\n",
      "Train Epoch: 7 [1432/2848 (50%)]\tLoss: 2.354580\n",
      "Train Epoch: 7 [1672/2848 (59%)]\tLoss: 2.658329\n",
      "Train Epoch: 7 [1912/2848 (67%)]\tLoss: 2.943598\n",
      "Train Epoch: 7 [2152/2848 (76%)]\tLoss: 2.174591\n",
      "Train Epoch: 7 [2392/2848 (84%)]\tLoss: 2.486573\n",
      "Train Epoch: 7 [2632/2848 (92%)]\tLoss: 2.213767\n",
      "\n",
      "Test set: Average loss: 0.4892, Accuracy: 67/200 (34%)\n",
      "\n",
      "Train Epoch: 8 [232/2848 (8%)]\tLoss: 2.921182\n",
      "Train Epoch: 8 [472/2848 (17%)]\tLoss: 2.588866\n",
      "Train Epoch: 8 [712/2848 (25%)]\tLoss: 2.419879\n",
      "Train Epoch: 8 [952/2848 (33%)]\tLoss: 2.890980\n",
      "Train Epoch: 8 [1192/2848 (42%)]\tLoss: 2.679806\n",
      "Train Epoch: 8 [1432/2848 (50%)]\tLoss: 2.483439\n",
      "Train Epoch: 8 [1672/2848 (59%)]\tLoss: 2.378509\n",
      "Train Epoch: 8 [1912/2848 (67%)]\tLoss: 2.351440\n",
      "Train Epoch: 8 [2152/2848 (76%)]\tLoss: 2.649083\n",
      "Train Epoch: 8 [2392/2848 (84%)]\tLoss: 2.218258\n",
      "Train Epoch: 8 [2632/2848 (92%)]\tLoss: 2.536969\n",
      "\n",
      "Test set: Average loss: 0.4875, Accuracy: 64/200 (32%)\n",
      "\n",
      "Train Epoch: 9 [232/2848 (8%)]\tLoss: 2.489892\n",
      "Train Epoch: 9 [472/2848 (17%)]\tLoss: 2.086346\n",
      "Train Epoch: 9 [712/2848 (25%)]\tLoss: 2.820306\n",
      "Train Epoch: 9 [952/2848 (33%)]\tLoss: 2.659489\n",
      "Train Epoch: 9 [1192/2848 (42%)]\tLoss: 2.376087\n",
      "Train Epoch: 9 [1432/2848 (50%)]\tLoss: 2.743827\n",
      "Train Epoch: 9 [1672/2848 (59%)]\tLoss: 2.292103\n",
      "Train Epoch: 9 [1912/2848 (67%)]\tLoss: 2.789849\n",
      "Train Epoch: 9 [2152/2848 (76%)]\tLoss: 2.386689\n",
      "Train Epoch: 9 [2392/2848 (84%)]\tLoss: 2.592753\n",
      "Train Epoch: 9 [2632/2848 (92%)]\tLoss: 2.489934\n",
      "\n",
      "Test set: Average loss: 0.5032, Accuracy: 69/200 (34%)\n",
      "\n",
      "Train Epoch: 10 [232/2848 (8%)]\tLoss: 2.131790\n",
      "Train Epoch: 10 [472/2848 (17%)]\tLoss: 2.346200\n",
      "Train Epoch: 10 [712/2848 (25%)]\tLoss: 2.554987\n",
      "Train Epoch: 10 [952/2848 (33%)]\tLoss: 2.539198\n",
      "Train Epoch: 10 [1192/2848 (42%)]\tLoss: 2.681084\n",
      "Train Epoch: 10 [1432/2848 (50%)]\tLoss: 2.826780\n",
      "Train Epoch: 10 [1672/2848 (59%)]\tLoss: 2.375068\n",
      "Train Epoch: 10 [1912/2848 (67%)]\tLoss: 2.811230\n",
      "Train Epoch: 10 [2152/2848 (76%)]\tLoss: 2.248954\n",
      "Train Epoch: 10 [2392/2848 (84%)]\tLoss: 2.191866\n",
      "Train Epoch: 10 [2632/2848 (92%)]\tLoss: 2.698431\n",
      "\n",
      "Test set: Average loss: 0.5608, Accuracy: 46/200 (23%)\n",
      "\n",
      "Train Epoch: 11 [232/2848 (8%)]\tLoss: 2.244981\n",
      "Train Epoch: 11 [472/2848 (17%)]\tLoss: 2.540304\n",
      "Train Epoch: 11 [712/2848 (25%)]\tLoss: 2.215176\n",
      "Train Epoch: 11 [952/2848 (33%)]\tLoss: 2.244473\n",
      "Train Epoch: 11 [1192/2848 (42%)]\tLoss: 2.400170\n",
      "Train Epoch: 11 [1432/2848 (50%)]\tLoss: 2.206277\n",
      "Train Epoch: 11 [1672/2848 (59%)]\tLoss: 2.921623\n",
      "Train Epoch: 11 [1912/2848 (67%)]\tLoss: 2.820836\n",
      "Train Epoch: 11 [2152/2848 (76%)]\tLoss: 2.399079\n",
      "Train Epoch: 11 [2392/2848 (84%)]\tLoss: 1.528508\n",
      "Train Epoch: 11 [2632/2848 (92%)]\tLoss: 2.334400\n",
      "\n",
      "Test set: Average loss: 0.4791, Accuracy: 68/200 (34%)\n",
      "\n",
      "Train Epoch: 12 [232/2848 (8%)]\tLoss: 2.327641\n",
      "Train Epoch: 12 [472/2848 (17%)]\tLoss: 2.549229\n",
      "Train Epoch: 12 [712/2848 (25%)]\tLoss: 2.265198\n",
      "Train Epoch: 12 [952/2848 (33%)]\tLoss: 1.800395\n",
      "Train Epoch: 12 [1192/2848 (42%)]\tLoss: 2.585052\n",
      "Train Epoch: 12 [1432/2848 (50%)]\tLoss: 1.767915\n",
      "Train Epoch: 12 [1672/2848 (59%)]\tLoss: 2.375246\n",
      "Train Epoch: 12 [1912/2848 (67%)]\tLoss: 2.207942\n",
      "Train Epoch: 12 [2152/2848 (76%)]\tLoss: 2.459665\n",
      "Train Epoch: 12 [2392/2848 (84%)]\tLoss: 2.478081\n",
      "Train Epoch: 12 [2632/2848 (92%)]\tLoss: 2.328757\n",
      "\n",
      "Test set: Average loss: 0.5443, Accuracy: 51/200 (26%)\n",
      "\n",
      "Train Epoch: 13 [232/2848 (8%)]\tLoss: 2.846111\n",
      "Train Epoch: 13 [472/2848 (17%)]\tLoss: 2.052624\n",
      "Train Epoch: 13 [712/2848 (25%)]\tLoss: 2.483599\n",
      "Train Epoch: 13 [952/2848 (33%)]\tLoss: 2.414506\n",
      "Train Epoch: 13 [1192/2848 (42%)]\tLoss: 2.385007\n",
      "Train Epoch: 13 [1432/2848 (50%)]\tLoss: 2.467290\n",
      "Train Epoch: 13 [1672/2848 (59%)]\tLoss: 2.139791\n",
      "Train Epoch: 13 [1912/2848 (67%)]\tLoss: 3.192101\n",
      "Train Epoch: 13 [2152/2848 (76%)]\tLoss: 2.257806\n",
      "Train Epoch: 13 [2392/2848 (84%)]\tLoss: 2.113359\n",
      "Train Epoch: 13 [2632/2848 (92%)]\tLoss: 2.212699\n",
      "\n",
      "Test set: Average loss: 0.4765, Accuracy: 66/200 (33%)\n",
      "\n",
      "Train Epoch: 14 [232/2848 (8%)]\tLoss: 3.021859\n",
      "Train Epoch: 14 [472/2848 (17%)]\tLoss: 2.698529\n",
      "Train Epoch: 14 [712/2848 (25%)]\tLoss: 2.075412\n",
      "Train Epoch: 14 [952/2848 (33%)]\tLoss: 1.699596\n",
      "Train Epoch: 14 [1192/2848 (42%)]\tLoss: 2.562632\n",
      "Train Epoch: 14 [1432/2848 (50%)]\tLoss: 2.429853\n",
      "Train Epoch: 14 [1672/2848 (59%)]\tLoss: 2.945585\n",
      "Train Epoch: 14 [1912/2848 (67%)]\tLoss: 2.234847\n",
      "Train Epoch: 14 [2152/2848 (76%)]\tLoss: 2.007565\n",
      "Train Epoch: 14 [2392/2848 (84%)]\tLoss: 2.323757\n",
      "Train Epoch: 14 [2632/2848 (92%)]\tLoss: 1.907184\n",
      "\n",
      "Test set: Average loss: 0.4865, Accuracy: 60/200 (30%)\n",
      "\n",
      "Train Epoch: 15 [232/2848 (8%)]\tLoss: 2.298505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [472/2848 (17%)]\tLoss: 2.539129\n",
      "Train Epoch: 15 [712/2848 (25%)]\tLoss: 2.283441\n",
      "Train Epoch: 15 [952/2848 (33%)]\tLoss: 2.770785\n",
      "Train Epoch: 15 [1192/2848 (42%)]\tLoss: 2.204360\n",
      "Train Epoch: 15 [1432/2848 (50%)]\tLoss: 2.590899\n",
      "Train Epoch: 15 [1672/2848 (59%)]\tLoss: 2.293252\n",
      "Train Epoch: 15 [1912/2848 (67%)]\tLoss: 1.795936\n",
      "Train Epoch: 15 [2152/2848 (76%)]\tLoss: 2.144001\n",
      "Train Epoch: 15 [2392/2848 (84%)]\tLoss: 2.105593\n",
      "Train Epoch: 15 [2632/2848 (92%)]\tLoss: 2.003122\n",
      "\n",
      "Test set: Average loss: 0.4616, Accuracy: 83/200 (42%)\n",
      "\n",
      "Train Epoch: 16 [232/2848 (8%)]\tLoss: 2.222526\n",
      "Train Epoch: 16 [472/2848 (17%)]\tLoss: 2.504892\n",
      "Train Epoch: 16 [712/2848 (25%)]\tLoss: 3.015508\n",
      "Train Epoch: 16 [952/2848 (33%)]\tLoss: 2.350688\n",
      "Train Epoch: 16 [1192/2848 (42%)]\tLoss: 1.758150\n",
      "Train Epoch: 16 [1432/2848 (50%)]\tLoss: 1.925414\n",
      "Train Epoch: 16 [1672/2848 (59%)]\tLoss: 2.395375\n",
      "Train Epoch: 16 [1912/2848 (67%)]\tLoss: 1.936930\n",
      "Train Epoch: 16 [2152/2848 (76%)]\tLoss: 3.065396\n",
      "Train Epoch: 16 [2392/2848 (84%)]\tLoss: 2.215340\n",
      "Train Epoch: 16 [2632/2848 (92%)]\tLoss: 2.382987\n",
      "\n",
      "Test set: Average loss: 0.4819, Accuracy: 70/200 (35%)\n",
      "\n",
      "Train Epoch: 17 [232/2848 (8%)]\tLoss: 2.846735\n",
      "Train Epoch: 17 [472/2848 (17%)]\tLoss: 2.216160\n",
      "Train Epoch: 17 [712/2848 (25%)]\tLoss: 2.676585\n",
      "Train Epoch: 17 [952/2848 (33%)]\tLoss: 2.179506\n",
      "Train Epoch: 17 [1192/2848 (42%)]\tLoss: 2.677351\n",
      "Train Epoch: 17 [1432/2848 (50%)]\tLoss: 1.528083\n",
      "Train Epoch: 17 [1672/2848 (59%)]\tLoss: 2.675325\n",
      "Train Epoch: 17 [1912/2848 (67%)]\tLoss: 1.977484\n",
      "Train Epoch: 17 [2152/2848 (76%)]\tLoss: 2.470251\n",
      "Train Epoch: 17 [2392/2848 (84%)]\tLoss: 1.804795\n",
      "Train Epoch: 17 [2632/2848 (92%)]\tLoss: 2.808796\n",
      "\n",
      "Test set: Average loss: 0.5013, Accuracy: 64/200 (32%)\n",
      "\n",
      "Train Epoch: 18 [232/2848 (8%)]\tLoss: 2.557404\n",
      "Train Epoch: 18 [472/2848 (17%)]\tLoss: 1.808536\n",
      "Train Epoch: 18 [712/2848 (25%)]\tLoss: 1.860601\n",
      "Train Epoch: 18 [952/2848 (33%)]\tLoss: 2.428575\n",
      "Train Epoch: 18 [1192/2848 (42%)]\tLoss: 2.710525\n",
      "Train Epoch: 18 [1432/2848 (50%)]\tLoss: 2.058845\n",
      "Train Epoch: 18 [1672/2848 (59%)]\tLoss: 1.936419\n",
      "Train Epoch: 18 [1912/2848 (67%)]\tLoss: 2.599177\n",
      "Train Epoch: 18 [2152/2848 (76%)]\tLoss: 1.973511\n",
      "Train Epoch: 18 [2392/2848 (84%)]\tLoss: 2.130660\n",
      "Train Epoch: 18 [2632/2848 (92%)]\tLoss: 1.710638\n",
      "\n",
      "Test set: Average loss: 0.4232, Accuracy: 102/200 (51%)\n",
      "\n",
      "Train Epoch: 19 [232/2848 (8%)]\tLoss: 1.866713\n",
      "Train Epoch: 19 [472/2848 (17%)]\tLoss: 2.476518\n",
      "Train Epoch: 19 [712/2848 (25%)]\tLoss: 2.188128\n",
      "Train Epoch: 19 [952/2848 (33%)]\tLoss: 1.723818\n",
      "Train Epoch: 19 [1192/2848 (42%)]\tLoss: 1.744852\n",
      "Train Epoch: 19 [1432/2848 (50%)]\tLoss: 1.824152\n",
      "Train Epoch: 19 [1672/2848 (59%)]\tLoss: 1.817976\n",
      "Train Epoch: 19 [1912/2848 (67%)]\tLoss: 2.046934\n",
      "Train Epoch: 19 [2152/2848 (76%)]\tLoss: 2.629470\n",
      "Train Epoch: 19 [2392/2848 (84%)]\tLoss: 2.095098\n",
      "Train Epoch: 19 [2632/2848 (92%)]\tLoss: 2.349021\n",
      "\n",
      "Test set: Average loss: 0.4421, Accuracy: 88/200 (44%)\n",
      "\n",
      "Train Epoch: 20 [232/2848 (8%)]\tLoss: 2.196682\n",
      "Train Epoch: 20 [472/2848 (17%)]\tLoss: 2.031018\n",
      "Train Epoch: 20 [712/2848 (25%)]\tLoss: 1.609393\n",
      "Train Epoch: 20 [952/2848 (33%)]\tLoss: 1.553500\n",
      "Train Epoch: 20 [1192/2848 (42%)]\tLoss: 2.229557\n",
      "Train Epoch: 20 [1432/2848 (50%)]\tLoss: 2.756021\n",
      "Train Epoch: 20 [1672/2848 (59%)]\tLoss: 2.273841\n",
      "Train Epoch: 20 [1912/2848 (67%)]\tLoss: 1.858159\n",
      "Train Epoch: 20 [2152/2848 (76%)]\tLoss: 2.280732\n",
      "Train Epoch: 20 [2392/2848 (84%)]\tLoss: 1.688995\n",
      "Train Epoch: 20 [2632/2848 (92%)]\tLoss: 1.741538\n",
      "\n",
      "Test set: Average loss: 0.5319, Accuracy: 71/200 (36%)\n",
      "\n",
      "Train Epoch: 21 [232/2848 (8%)]\tLoss: 2.186079\n",
      "Train Epoch: 21 [472/2848 (17%)]\tLoss: 2.037340\n",
      "Train Epoch: 21 [712/2848 (25%)]\tLoss: 2.173864\n",
      "Train Epoch: 21 [952/2848 (33%)]\tLoss: 2.574602\n",
      "Train Epoch: 21 [1192/2848 (42%)]\tLoss: 2.554824\n",
      "Train Epoch: 21 [1432/2848 (50%)]\tLoss: 1.595822\n",
      "Train Epoch: 21 [1672/2848 (59%)]\tLoss: 2.053575\n",
      "Train Epoch: 21 [1912/2848 (67%)]\tLoss: 2.523517\n",
      "Train Epoch: 21 [2152/2848 (76%)]\tLoss: 2.186325\n",
      "Train Epoch: 21 [2392/2848 (84%)]\tLoss: 2.457761\n",
      "Train Epoch: 21 [2632/2848 (92%)]\tLoss: 2.426213\n",
      "\n",
      "Test set: Average loss: 0.4764, Accuracy: 82/200 (41%)\n",
      "\n",
      "Train Epoch: 22 [232/2848 (8%)]\tLoss: 1.796822\n",
      "Train Epoch: 22 [472/2848 (17%)]\tLoss: 1.904563\n",
      "Train Epoch: 22 [712/2848 (25%)]\tLoss: 1.429442\n",
      "Train Epoch: 22 [952/2848 (33%)]\tLoss: 1.754843\n",
      "Train Epoch: 22 [1192/2848 (42%)]\tLoss: 1.986434\n",
      "Train Epoch: 22 [1432/2848 (50%)]\tLoss: 2.647089\n",
      "Train Epoch: 22 [1672/2848 (59%)]\tLoss: 2.129646\n",
      "Train Epoch: 22 [1912/2848 (67%)]\tLoss: 1.499864\n",
      "Train Epoch: 22 [2152/2848 (76%)]\tLoss: 2.278517\n",
      "Train Epoch: 22 [2392/2848 (84%)]\tLoss: 1.542772\n",
      "Train Epoch: 22 [2632/2848 (92%)]\tLoss: 1.376047\n",
      "\n",
      "Test set: Average loss: 0.4945, Accuracy: 88/200 (44%)\n",
      "\n",
      "Train Epoch: 23 [232/2848 (8%)]\tLoss: 2.387196\n",
      "Train Epoch: 23 [472/2848 (17%)]\tLoss: 2.073046\n",
      "Train Epoch: 23 [712/2848 (25%)]\tLoss: 2.086081\n",
      "Train Epoch: 23 [952/2848 (33%)]\tLoss: 1.871272\n",
      "Train Epoch: 23 [1192/2848 (42%)]\tLoss: 1.858451\n",
      "Train Epoch: 23 [1432/2848 (50%)]\tLoss: 1.826875\n",
      "Train Epoch: 23 [1672/2848 (59%)]\tLoss: 2.165612\n",
      "Train Epoch: 23 [1912/2848 (67%)]\tLoss: 1.806055\n",
      "Train Epoch: 23 [2152/2848 (76%)]\tLoss: 1.688262\n",
      "Train Epoch: 23 [2392/2848 (84%)]\tLoss: 1.827421\n",
      "Train Epoch: 23 [2632/2848 (92%)]\tLoss: 2.230272\n",
      "\n",
      "Test set: Average loss: 0.5132, Accuracy: 80/200 (40%)\n",
      "\n",
      "Train Epoch: 24 [232/2848 (8%)]\tLoss: 2.004891\n",
      "Train Epoch: 24 [472/2848 (17%)]\tLoss: 2.080838\n",
      "Train Epoch: 24 [712/2848 (25%)]\tLoss: 2.309751\n",
      "Train Epoch: 24 [952/2848 (33%)]\tLoss: 2.520725\n",
      "Train Epoch: 24 [1192/2848 (42%)]\tLoss: 1.910240\n",
      "Train Epoch: 24 [1432/2848 (50%)]\tLoss: 2.317263\n",
      "Train Epoch: 24 [1672/2848 (59%)]\tLoss: 2.156951\n",
      "Train Epoch: 24 [1912/2848 (67%)]\tLoss: 2.000759\n",
      "Train Epoch: 24 [2152/2848 (76%)]\tLoss: 1.561099\n",
      "Train Epoch: 24 [2392/2848 (84%)]\tLoss: 2.051007\n",
      "Train Epoch: 24 [2632/2848 (92%)]\tLoss: 2.068435\n",
      "\n",
      "Test set: Average loss: 1.0085, Accuracy: 79/200 (40%)\n",
      "\n",
      "Train Epoch: 25 [232/2848 (8%)]\tLoss: 2.298915\n",
      "Train Epoch: 25 [472/2848 (17%)]\tLoss: 1.668128\n",
      "Train Epoch: 25 [712/2848 (25%)]\tLoss: 2.016889\n",
      "Train Epoch: 25 [952/2848 (33%)]\tLoss: 2.148258\n",
      "Train Epoch: 25 [1192/2848 (42%)]\tLoss: 2.868800\n",
      "Train Epoch: 25 [1432/2848 (50%)]\tLoss: 1.875829\n",
      "Train Epoch: 25 [1672/2848 (59%)]\tLoss: 2.368224\n",
      "Train Epoch: 25 [1912/2848 (67%)]\tLoss: 2.086874\n",
      "Train Epoch: 25 [2152/2848 (76%)]\tLoss: 1.713026\n",
      "Train Epoch: 25 [2392/2848 (84%)]\tLoss: 1.978436\n",
      "Train Epoch: 25 [2632/2848 (92%)]\tLoss: 1.745658\n",
      "\n",
      "Test set: Average loss: 0.4847, Accuracy: 85/200 (42%)\n",
      "\n",
      "Train Epoch: 26 [232/2848 (8%)]\tLoss: 1.856612\n",
      "Train Epoch: 26 [472/2848 (17%)]\tLoss: 1.388432\n",
      "Train Epoch: 26 [712/2848 (25%)]\tLoss: 2.398138\n",
      "Train Epoch: 26 [952/2848 (33%)]\tLoss: 1.826859\n",
      "Train Epoch: 26 [1192/2848 (42%)]\tLoss: 1.770397\n",
      "Train Epoch: 26 [1432/2848 (50%)]\tLoss: 2.187238\n",
      "Train Epoch: 26 [1672/2848 (59%)]\tLoss: 1.644282\n",
      "Train Epoch: 26 [1912/2848 (67%)]\tLoss: 2.357728\n",
      "Train Epoch: 26 [2152/2848 (76%)]\tLoss: 1.901350\n",
      "Train Epoch: 26 [2392/2848 (84%)]\tLoss: 1.562359\n",
      "Train Epoch: 26 [2632/2848 (92%)]\tLoss: 1.488929\n",
      "\n",
      "Test set: Average loss: 0.4657, Accuracy: 105/200 (52%)\n",
      "\n",
      "Train Epoch: 27 [232/2848 (8%)]\tLoss: 1.740164\n",
      "Train Epoch: 27 [472/2848 (17%)]\tLoss: 1.859799\n",
      "Train Epoch: 27 [712/2848 (25%)]\tLoss: 2.455405\n",
      "Train Epoch: 27 [952/2848 (33%)]\tLoss: 2.269260\n",
      "Train Epoch: 27 [1192/2848 (42%)]\tLoss: 1.861822\n",
      "Train Epoch: 27 [1432/2848 (50%)]\tLoss: 1.511267\n",
      "Train Epoch: 27 [1672/2848 (59%)]\tLoss: 1.967094\n",
      "Train Epoch: 27 [1912/2848 (67%)]\tLoss: 2.429648\n",
      "Train Epoch: 27 [2152/2848 (76%)]\tLoss: 3.204831\n",
      "Train Epoch: 27 [2392/2848 (84%)]\tLoss: 1.225046\n",
      "Train Epoch: 27 [2632/2848 (92%)]\tLoss: 1.141688\n",
      "\n",
      "Test set: Average loss: 0.4247, Accuracy: 106/200 (53%)\n",
      "\n",
      "Train Epoch: 28 [232/2848 (8%)]\tLoss: 1.840232\n",
      "Train Epoch: 28 [472/2848 (17%)]\tLoss: 1.328869\n",
      "Train Epoch: 28 [712/2848 (25%)]\tLoss: 1.927725\n",
      "Train Epoch: 28 [952/2848 (33%)]\tLoss: 1.645059\n",
      "Train Epoch: 28 [1192/2848 (42%)]\tLoss: 1.819362\n",
      "Train Epoch: 28 [1432/2848 (50%)]\tLoss: 2.098101\n",
      "Train Epoch: 28 [1672/2848 (59%)]\tLoss: 1.934439\n",
      "Train Epoch: 28 [1912/2848 (67%)]\tLoss: 1.581571\n",
      "Train Epoch: 28 [2152/2848 (76%)]\tLoss: 1.751116\n",
      "Train Epoch: 28 [2392/2848 (84%)]\tLoss: 1.788043\n",
      "Train Epoch: 28 [2632/2848 (92%)]\tLoss: 2.714076\n",
      "\n",
      "Test set: Average loss: 0.4089, Accuracy: 124/200 (62%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [232/2848 (8%)]\tLoss: 1.604497\n",
      "Train Epoch: 29 [472/2848 (17%)]\tLoss: 1.828484\n",
      "Train Epoch: 29 [712/2848 (25%)]\tLoss: 2.254837\n",
      "Train Epoch: 29 [952/2848 (33%)]\tLoss: 1.899946\n",
      "Train Epoch: 29 [1192/2848 (42%)]\tLoss: 1.664507\n",
      "Train Epoch: 29 [1432/2848 (50%)]\tLoss: 1.857858\n",
      "Train Epoch: 29 [1672/2848 (59%)]\tLoss: 1.788832\n",
      "Train Epoch: 29 [1912/2848 (67%)]\tLoss: 1.544524\n",
      "Train Epoch: 29 [2152/2848 (76%)]\tLoss: 3.010834\n",
      "Train Epoch: 29 [2392/2848 (84%)]\tLoss: 1.698916\n",
      "Train Epoch: 29 [2632/2848 (92%)]\tLoss: 1.969468\n",
      "\n",
      "Test set: Average loss: 0.5553, Accuracy: 90/200 (45%)\n",
      "\n",
      "Train Epoch: 30 [232/2848 (8%)]\tLoss: 1.784714\n",
      "Train Epoch: 30 [472/2848 (17%)]\tLoss: 1.063007\n",
      "Train Epoch: 30 [712/2848 (25%)]\tLoss: 1.774217\n",
      "Train Epoch: 30 [952/2848 (33%)]\tLoss: 2.122136\n",
      "Train Epoch: 30 [1192/2848 (42%)]\tLoss: 1.658652\n",
      "Train Epoch: 30 [1432/2848 (50%)]\tLoss: 1.685480\n",
      "Train Epoch: 30 [1672/2848 (59%)]\tLoss: 1.679894\n",
      "Train Epoch: 30 [1912/2848 (67%)]\tLoss: 1.311632\n",
      "Train Epoch: 30 [2152/2848 (76%)]\tLoss: 1.368333\n",
      "Train Epoch: 30 [2392/2848 (84%)]\tLoss: 1.259804\n",
      "Train Epoch: 30 [2632/2848 (92%)]\tLoss: 1.713166\n",
      "\n",
      "Test set: Average loss: 0.5147, Accuracy: 91/200 (46%)\n",
      "\n",
      "Train Epoch: 31 [232/2848 (8%)]\tLoss: 1.632556\n",
      "Train Epoch: 31 [472/2848 (17%)]\tLoss: 2.635953\n",
      "Train Epoch: 31 [712/2848 (25%)]\tLoss: 1.226104\n",
      "Train Epoch: 31 [952/2848 (33%)]\tLoss: 1.267417\n",
      "Train Epoch: 31 [1192/2848 (42%)]\tLoss: 1.505666\n",
      "Train Epoch: 31 [1432/2848 (50%)]\tLoss: 1.875996\n",
      "Train Epoch: 31 [1672/2848 (59%)]\tLoss: 2.002190\n",
      "Train Epoch: 31 [1912/2848 (67%)]\tLoss: 1.585867\n",
      "Train Epoch: 31 [2152/2848 (76%)]\tLoss: 1.719102\n",
      "Train Epoch: 31 [2392/2848 (84%)]\tLoss: 1.889614\n",
      "Train Epoch: 31 [2632/2848 (92%)]\tLoss: 1.630471\n",
      "\n",
      "Test set: Average loss: 0.5872, Accuracy: 87/200 (44%)\n",
      "\n",
      "Train Epoch: 32 [232/2848 (8%)]\tLoss: 1.285305\n",
      "Train Epoch: 32 [472/2848 (17%)]\tLoss: 1.479478\n",
      "Train Epoch: 32 [712/2848 (25%)]\tLoss: 1.229872\n",
      "Train Epoch: 32 [952/2848 (33%)]\tLoss: 1.274604\n",
      "Train Epoch: 32 [1192/2848 (42%)]\tLoss: 1.505897\n",
      "Train Epoch: 32 [1432/2848 (50%)]\tLoss: 1.062510\n",
      "Train Epoch: 32 [1672/2848 (59%)]\tLoss: 0.865711\n",
      "Train Epoch: 32 [1912/2848 (67%)]\tLoss: 1.765983\n",
      "Train Epoch: 32 [2152/2848 (76%)]\tLoss: 1.437459\n",
      "Train Epoch: 32 [2392/2848 (84%)]\tLoss: 1.579873\n",
      "Train Epoch: 32 [2632/2848 (92%)]\tLoss: 1.931542\n",
      "\n",
      "Test set: Average loss: 0.5305, Accuracy: 105/200 (52%)\n",
      "\n",
      "Train Epoch: 33 [232/2848 (8%)]\tLoss: 1.606454\n",
      "Train Epoch: 33 [472/2848 (17%)]\tLoss: 1.668790\n",
      "Train Epoch: 33 [712/2848 (25%)]\tLoss: 1.457619\n",
      "Train Epoch: 33 [952/2848 (33%)]\tLoss: 1.538781\n",
      "Train Epoch: 33 [1192/2848 (42%)]\tLoss: 1.551342\n",
      "Train Epoch: 33 [1432/2848 (50%)]\tLoss: 1.967705\n",
      "Train Epoch: 33 [1672/2848 (59%)]\tLoss: 1.531796\n",
      "Train Epoch: 33 [1912/2848 (67%)]\tLoss: 2.113147\n",
      "Train Epoch: 33 [2152/2848 (76%)]\tLoss: 1.974626\n",
      "Train Epoch: 33 [2392/2848 (84%)]\tLoss: 1.188869\n",
      "Train Epoch: 33 [2632/2848 (92%)]\tLoss: 1.779030\n",
      "\n",
      "Test set: Average loss: 0.4067, Accuracy: 109/200 (54%)\n",
      "\n",
      "Train Epoch: 34 [232/2848 (8%)]\tLoss: 1.230508\n",
      "Train Epoch: 34 [472/2848 (17%)]\tLoss: 1.746082\n",
      "Train Epoch: 34 [712/2848 (25%)]\tLoss: 1.756292\n",
      "Train Epoch: 34 [952/2848 (33%)]\tLoss: 2.006526\n",
      "Train Epoch: 34 [1192/2848 (42%)]\tLoss: 1.592670\n",
      "Train Epoch: 34 [1432/2848 (50%)]\tLoss: 2.350639\n",
      "Train Epoch: 34 [1672/2848 (59%)]\tLoss: 1.635107\n",
      "Train Epoch: 34 [1912/2848 (67%)]\tLoss: 1.690811\n",
      "Train Epoch: 34 [2152/2848 (76%)]\tLoss: 1.620698\n",
      "Train Epoch: 34 [2392/2848 (84%)]\tLoss: 1.018878\n",
      "Train Epoch: 34 [2632/2848 (92%)]\tLoss: 1.492649\n",
      "\n",
      "Test set: Average loss: 0.5602, Accuracy: 93/200 (46%)\n",
      "\n",
      "Train Epoch: 35 [232/2848 (8%)]\tLoss: 1.308559\n",
      "Train Epoch: 35 [472/2848 (17%)]\tLoss: 1.781435\n",
      "Train Epoch: 35 [712/2848 (25%)]\tLoss: 1.237724\n",
      "Train Epoch: 35 [952/2848 (33%)]\tLoss: 1.798487\n",
      "Train Epoch: 35 [1192/2848 (42%)]\tLoss: 1.413619\n",
      "Train Epoch: 35 [1432/2848 (50%)]\tLoss: 1.209986\n",
      "Train Epoch: 35 [1672/2848 (59%)]\tLoss: 1.343847\n",
      "Train Epoch: 35 [1912/2848 (67%)]\tLoss: 1.152196\n",
      "Train Epoch: 35 [2152/2848 (76%)]\tLoss: 1.096925\n",
      "Train Epoch: 35 [2392/2848 (84%)]\tLoss: 0.968725\n",
      "Train Epoch: 35 [2632/2848 (92%)]\tLoss: 1.751296\n",
      "\n",
      "Test set: Average loss: 0.4700, Accuracy: 116/200 (58%)\n",
      "\n",
      "Train Epoch: 36 [232/2848 (8%)]\tLoss: 1.726367\n",
      "Train Epoch: 36 [472/2848 (17%)]\tLoss: 1.218514\n",
      "Train Epoch: 36 [712/2848 (25%)]\tLoss: 1.602941\n",
      "Train Epoch: 36 [952/2848 (33%)]\tLoss: 0.973700\n",
      "Train Epoch: 36 [1192/2848 (42%)]\tLoss: 1.284672\n",
      "Train Epoch: 36 [1432/2848 (50%)]\tLoss: 1.964377\n",
      "Train Epoch: 36 [1672/2848 (59%)]\tLoss: 1.646248\n",
      "Train Epoch: 36 [1912/2848 (67%)]\tLoss: 1.930661\n",
      "Train Epoch: 36 [2152/2848 (76%)]\tLoss: 1.288003\n",
      "Train Epoch: 36 [2392/2848 (84%)]\tLoss: 1.237621\n",
      "Train Epoch: 36 [2632/2848 (92%)]\tLoss: 2.296307\n",
      "\n",
      "Test set: Average loss: 0.4237, Accuracy: 117/200 (58%)\n",
      "\n",
      "Train Epoch: 37 [232/2848 (8%)]\tLoss: 1.612113\n",
      "Train Epoch: 37 [472/2848 (17%)]\tLoss: 1.257682\n",
      "Train Epoch: 37 [712/2848 (25%)]\tLoss: 1.601201\n",
      "Train Epoch: 37 [952/2848 (33%)]\tLoss: 1.393784\n",
      "Train Epoch: 37 [1192/2848 (42%)]\tLoss: 1.322667\n",
      "Train Epoch: 37 [1432/2848 (50%)]\tLoss: 1.256095\n",
      "Train Epoch: 37 [1672/2848 (59%)]\tLoss: 1.376451\n",
      "Train Epoch: 37 [1912/2848 (67%)]\tLoss: 1.565572\n",
      "Train Epoch: 37 [2152/2848 (76%)]\tLoss: 1.176954\n",
      "Train Epoch: 37 [2392/2848 (84%)]\tLoss: 2.174104\n",
      "Train Epoch: 37 [2632/2848 (92%)]\tLoss: 1.283793\n",
      "\n",
      "Test set: Average loss: 0.3875, Accuracy: 118/200 (59%)\n",
      "\n",
      "Train Epoch: 38 [232/2848 (8%)]\tLoss: 1.755759\n",
      "Train Epoch: 38 [472/2848 (17%)]\tLoss: 1.344748\n",
      "Train Epoch: 38 [712/2848 (25%)]\tLoss: 2.427693\n",
      "Train Epoch: 38 [952/2848 (33%)]\tLoss: 1.850189\n",
      "Train Epoch: 38 [1192/2848 (42%)]\tLoss: 2.348452\n",
      "Train Epoch: 38 [1432/2848 (50%)]\tLoss: 1.026735\n",
      "Train Epoch: 38 [1672/2848 (59%)]\tLoss: 0.818557\n",
      "Train Epoch: 38 [1912/2848 (67%)]\tLoss: 0.738848\n",
      "Train Epoch: 38 [2152/2848 (76%)]\tLoss: 1.366130\n",
      "Train Epoch: 38 [2392/2848 (84%)]\tLoss: 2.331275\n",
      "Train Epoch: 38 [2632/2848 (92%)]\tLoss: 1.894012\n",
      "\n",
      "Test set: Average loss: 0.3686, Accuracy: 132/200 (66%)\n",
      "\n",
      "Train Epoch: 39 [232/2848 (8%)]\tLoss: 0.968810\n",
      "Train Epoch: 39 [472/2848 (17%)]\tLoss: 2.032546\n",
      "Train Epoch: 39 [712/2848 (25%)]\tLoss: 1.176501\n",
      "Train Epoch: 39 [952/2848 (33%)]\tLoss: 0.598921\n",
      "Train Epoch: 39 [1192/2848 (42%)]\tLoss: 1.754021\n",
      "Train Epoch: 39 [1432/2848 (50%)]\tLoss: 1.149811\n",
      "Train Epoch: 39 [1672/2848 (59%)]\tLoss: 2.146700\n",
      "Train Epoch: 39 [1912/2848 (67%)]\tLoss: 1.300315\n",
      "Train Epoch: 39 [2152/2848 (76%)]\tLoss: 1.508057\n",
      "Train Epoch: 39 [2392/2848 (84%)]\tLoss: 0.877628\n",
      "Train Epoch: 39 [2632/2848 (92%)]\tLoss: 0.924825\n",
      "\n",
      "Test set: Average loss: 0.4021, Accuracy: 118/200 (59%)\n",
      "\n",
      "Train Epoch: 40 [232/2848 (8%)]\tLoss: 1.696957\n",
      "Train Epoch: 40 [472/2848 (17%)]\tLoss: 1.655814\n",
      "Train Epoch: 40 [712/2848 (25%)]\tLoss: 0.997239\n",
      "Train Epoch: 40 [952/2848 (33%)]\tLoss: 1.996271\n",
      "Train Epoch: 40 [1192/2848 (42%)]\tLoss: 1.398473\n",
      "Train Epoch: 40 [1432/2848 (50%)]\tLoss: 1.294363\n",
      "Train Epoch: 40 [1672/2848 (59%)]\tLoss: 1.044470\n",
      "Train Epoch: 40 [1912/2848 (67%)]\tLoss: 2.431947\n",
      "Train Epoch: 40 [2152/2848 (76%)]\tLoss: 1.466213\n",
      "Train Epoch: 40 [2392/2848 (84%)]\tLoss: 1.715445\n",
      "Train Epoch: 40 [2632/2848 (92%)]\tLoss: 1.131952\n",
      "\n",
      "Test set: Average loss: 0.4000, Accuracy: 113/200 (56%)\n",
      "\n",
      "Train Epoch: 41 [232/2848 (8%)]\tLoss: 1.640283\n",
      "Train Epoch: 41 [472/2848 (17%)]\tLoss: 1.575930\n",
      "Train Epoch: 41 [712/2848 (25%)]\tLoss: 1.249802\n",
      "Train Epoch: 41 [952/2848 (33%)]\tLoss: 1.883104\n",
      "Train Epoch: 41 [1192/2848 (42%)]\tLoss: 1.286603\n",
      "Train Epoch: 41 [1432/2848 (50%)]\tLoss: 1.273690\n",
      "Train Epoch: 41 [1672/2848 (59%)]\tLoss: 1.673649\n",
      "Train Epoch: 41 [1912/2848 (67%)]\tLoss: 1.570113\n",
      "Train Epoch: 41 [2152/2848 (76%)]\tLoss: 1.230106\n",
      "Train Epoch: 41 [2392/2848 (84%)]\tLoss: 0.969071\n",
      "Train Epoch: 41 [2632/2848 (92%)]\tLoss: 1.085397\n",
      "\n",
      "Test set: Average loss: 0.4892, Accuracy: 111/200 (56%)\n",
      "\n",
      "Train Epoch: 42 [232/2848 (8%)]\tLoss: 1.369818\n",
      "Train Epoch: 42 [472/2848 (17%)]\tLoss: 0.608064\n",
      "Train Epoch: 42 [712/2848 (25%)]\tLoss: 0.589009\n",
      "Train Epoch: 42 [952/2848 (33%)]\tLoss: 1.439752\n",
      "Train Epoch: 42 [1192/2848 (42%)]\tLoss: 1.026619\n",
      "Train Epoch: 42 [1432/2848 (50%)]\tLoss: 1.276218\n",
      "Train Epoch: 42 [1672/2848 (59%)]\tLoss: 0.897472\n",
      "Train Epoch: 42 [1912/2848 (67%)]\tLoss: 1.245323\n",
      "Train Epoch: 42 [2152/2848 (76%)]\tLoss: 1.245991\n",
      "Train Epoch: 42 [2392/2848 (84%)]\tLoss: 1.100844\n",
      "Train Epoch: 42 [2632/2848 (92%)]\tLoss: 1.477196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5341, Accuracy: 112/200 (56%)\n",
      "\n",
      "Train Epoch: 43 [232/2848 (8%)]\tLoss: 2.253450\n",
      "Train Epoch: 43 [472/2848 (17%)]\tLoss: 1.045686\n",
      "Train Epoch: 43 [712/2848 (25%)]\tLoss: 1.051376\n",
      "Train Epoch: 43 [952/2848 (33%)]\tLoss: 1.420443\n",
      "Train Epoch: 43 [1192/2848 (42%)]\tLoss: 0.932774\n",
      "Train Epoch: 43 [1432/2848 (50%)]\tLoss: 0.749734\n",
      "Train Epoch: 43 [1672/2848 (59%)]\tLoss: 0.924905\n",
      "Train Epoch: 43 [1912/2848 (67%)]\tLoss: 1.345502\n",
      "Train Epoch: 43 [2152/2848 (76%)]\tLoss: 1.388630\n",
      "Train Epoch: 43 [2392/2848 (84%)]\tLoss: 1.044001\n",
      "Train Epoch: 43 [2632/2848 (92%)]\tLoss: 1.865926\n",
      "\n",
      "Test set: Average loss: 0.5118, Accuracy: 126/200 (63%)\n",
      "\n",
      "Train Epoch: 44 [232/2848 (8%)]\tLoss: 1.217006\n",
      "Train Epoch: 44 [472/2848 (17%)]\tLoss: 1.294876\n",
      "Train Epoch: 44 [712/2848 (25%)]\tLoss: 0.794219\n",
      "Train Epoch: 44 [952/2848 (33%)]\tLoss: 1.282965\n",
      "Train Epoch: 44 [1192/2848 (42%)]\tLoss: 0.786331\n",
      "Train Epoch: 44 [1432/2848 (50%)]\tLoss: 2.038388\n",
      "Train Epoch: 44 [1672/2848 (59%)]\tLoss: 0.945497\n",
      "Train Epoch: 44 [1912/2848 (67%)]\tLoss: 1.150070\n",
      "Train Epoch: 44 [2152/2848 (76%)]\tLoss: 1.431935\n",
      "Train Epoch: 44 [2392/2848 (84%)]\tLoss: 1.707717\n",
      "Train Epoch: 44 [2632/2848 (92%)]\tLoss: 0.891163\n",
      "\n",
      "Test set: Average loss: 0.3880, Accuracy: 130/200 (65%)\n",
      "\n",
      "Train Epoch: 45 [232/2848 (8%)]\tLoss: 0.686718\n",
      "Train Epoch: 45 [472/2848 (17%)]\tLoss: 1.058187\n",
      "Train Epoch: 45 [712/2848 (25%)]\tLoss: 0.671029\n",
      "Train Epoch: 45 [952/2848 (33%)]\tLoss: 1.018678\n",
      "Train Epoch: 45 [1192/2848 (42%)]\tLoss: 0.846464\n",
      "Train Epoch: 45 [1432/2848 (50%)]\tLoss: 1.273090\n",
      "Train Epoch: 45 [1672/2848 (59%)]\tLoss: 0.570700\n",
      "Train Epoch: 45 [1912/2848 (67%)]\tLoss: 1.276878\n",
      "Train Epoch: 45 [2152/2848 (76%)]\tLoss: 1.248965\n",
      "Train Epoch: 45 [2392/2848 (84%)]\tLoss: 1.373449\n",
      "Train Epoch: 45 [2632/2848 (92%)]\tLoss: 0.953349\n",
      "\n",
      "Test set: Average loss: 0.4752, Accuracy: 130/200 (65%)\n",
      "\n",
      "Train Epoch: 46 [232/2848 (8%)]\tLoss: 0.775804\n",
      "Train Epoch: 46 [472/2848 (17%)]\tLoss: 1.277843\n",
      "Train Epoch: 46 [712/2848 (25%)]\tLoss: 0.921105\n",
      "Train Epoch: 46 [952/2848 (33%)]\tLoss: 1.154619\n",
      "Train Epoch: 46 [1192/2848 (42%)]\tLoss: 1.171603\n",
      "Train Epoch: 46 [1432/2848 (50%)]\tLoss: 0.954255\n",
      "Train Epoch: 46 [1672/2848 (59%)]\tLoss: 0.770907\n",
      "Train Epoch: 46 [1912/2848 (67%)]\tLoss: 0.454143\n",
      "Train Epoch: 46 [2152/2848 (76%)]\tLoss: 1.482081\n",
      "Train Epoch: 46 [2392/2848 (84%)]\tLoss: 1.119684\n",
      "Train Epoch: 46 [2632/2848 (92%)]\tLoss: 0.842568\n",
      "\n",
      "Test set: Average loss: 0.3978, Accuracy: 142/200 (71%)\n",
      "\n",
      "Train Epoch: 47 [232/2848 (8%)]\tLoss: 1.341253\n",
      "Train Epoch: 47 [472/2848 (17%)]\tLoss: 0.751743\n",
      "Train Epoch: 47 [712/2848 (25%)]\tLoss: 1.125739\n",
      "Train Epoch: 47 [952/2848 (33%)]\tLoss: 0.885248\n",
      "Train Epoch: 47 [1192/2848 (42%)]\tLoss: 1.337393\n",
      "Train Epoch: 47 [1432/2848 (50%)]\tLoss: 1.728705\n",
      "Train Epoch: 47 [1672/2848 (59%)]\tLoss: 0.820120\n",
      "Train Epoch: 47 [1912/2848 (67%)]\tLoss: 1.096258\n",
      "Train Epoch: 47 [2152/2848 (76%)]\tLoss: 0.688761\n",
      "Train Epoch: 47 [2392/2848 (84%)]\tLoss: 0.835916\n",
      "Train Epoch: 47 [2632/2848 (92%)]\tLoss: 1.442719\n",
      "\n",
      "Test set: Average loss: 0.4625, Accuracy: 136/200 (68%)\n",
      "\n",
      "Train Epoch: 48 [232/2848 (8%)]\tLoss: 0.599629\n",
      "Train Epoch: 48 [472/2848 (17%)]\tLoss: 0.714096\n",
      "Train Epoch: 48 [712/2848 (25%)]\tLoss: 0.918725\n",
      "Train Epoch: 48 [952/2848 (33%)]\tLoss: 1.182133\n",
      "Train Epoch: 48 [1192/2848 (42%)]\tLoss: 0.908256\n",
      "Train Epoch: 48 [1432/2848 (50%)]\tLoss: 1.108830\n",
      "Train Epoch: 48 [1672/2848 (59%)]\tLoss: 0.602893\n",
      "Train Epoch: 48 [1912/2848 (67%)]\tLoss: 0.796398\n",
      "Train Epoch: 48 [2152/2848 (76%)]\tLoss: 0.657104\n",
      "Train Epoch: 48 [2392/2848 (84%)]\tLoss: 0.903768\n",
      "Train Epoch: 48 [2632/2848 (92%)]\tLoss: 1.039829\n",
      "\n",
      "Test set: Average loss: 0.4489, Accuracy: 126/200 (63%)\n",
      "\n",
      "Train Epoch: 49 [232/2848 (8%)]\tLoss: 1.028087\n",
      "Train Epoch: 49 [472/2848 (17%)]\tLoss: 0.712908\n",
      "Train Epoch: 49 [712/2848 (25%)]\tLoss: 0.561316\n",
      "Train Epoch: 49 [952/2848 (33%)]\tLoss: 1.301035\n",
      "Train Epoch: 49 [1192/2848 (42%)]\tLoss: 0.889341\n",
      "Train Epoch: 49 [1432/2848 (50%)]\tLoss: 0.953809\n",
      "Train Epoch: 49 [1672/2848 (59%)]\tLoss: 1.083380\n",
      "Train Epoch: 49 [1912/2848 (67%)]\tLoss: 0.941767\n",
      "Train Epoch: 49 [2152/2848 (76%)]\tLoss: 1.401475\n",
      "Train Epoch: 49 [2392/2848 (84%)]\tLoss: 0.343056\n",
      "Train Epoch: 49 [2632/2848 (92%)]\tLoss: 0.492650\n",
      "\n",
      "Test set: Average loss: 0.4618, Accuracy: 135/200 (68%)\n",
      "\n",
      "Train Epoch: 50 [232/2848 (8%)]\tLoss: 0.502610\n",
      "Train Epoch: 50 [472/2848 (17%)]\tLoss: 1.222208\n",
      "Train Epoch: 50 [712/2848 (25%)]\tLoss: 0.657609\n",
      "Train Epoch: 50 [952/2848 (33%)]\tLoss: 0.881400\n",
      "Train Epoch: 50 [1192/2848 (42%)]\tLoss: 1.357279\n",
      "Train Epoch: 50 [1432/2848 (50%)]\tLoss: 1.017513\n",
      "Train Epoch: 50 [1672/2848 (59%)]\tLoss: 1.378075\n",
      "Train Epoch: 50 [1912/2848 (67%)]\tLoss: 0.815264\n",
      "Train Epoch: 50 [2152/2848 (76%)]\tLoss: 1.703394\n",
      "Train Epoch: 50 [2392/2848 (84%)]\tLoss: 0.768230\n",
      "Train Epoch: 50 [2632/2848 (92%)]\tLoss: 0.530744\n",
      "\n",
      "Test set: Average loss: 0.4797, Accuracy: 144/200 (72%)\n",
      "\n",
      "Train Epoch: 51 [232/2848 (8%)]\tLoss: 1.223526\n",
      "Train Epoch: 51 [472/2848 (17%)]\tLoss: 0.314226\n",
      "Train Epoch: 51 [712/2848 (25%)]\tLoss: 1.158285\n",
      "Train Epoch: 51 [952/2848 (33%)]\tLoss: 1.655425\n",
      "Train Epoch: 51 [1192/2848 (42%)]\tLoss: 0.800084\n",
      "Train Epoch: 51 [1432/2848 (50%)]\tLoss: 1.314218\n",
      "Train Epoch: 51 [1672/2848 (59%)]\tLoss: 1.199765\n",
      "Train Epoch: 51 [1912/2848 (67%)]\tLoss: 0.676440\n",
      "Train Epoch: 51 [2152/2848 (76%)]\tLoss: 0.683952\n",
      "Train Epoch: 51 [2392/2848 (84%)]\tLoss: 0.639849\n",
      "Train Epoch: 51 [2632/2848 (92%)]\tLoss: 0.750882\n",
      "\n",
      "Test set: Average loss: 0.3715, Accuracy: 144/200 (72%)\n",
      "\n",
      "Train Epoch: 52 [232/2848 (8%)]\tLoss: 1.733826\n",
      "Train Epoch: 52 [472/2848 (17%)]\tLoss: 0.988890\n",
      "Train Epoch: 52 [712/2848 (25%)]\tLoss: 1.000458\n",
      "Train Epoch: 52 [952/2848 (33%)]\tLoss: 0.705278\n",
      "Train Epoch: 52 [1192/2848 (42%)]\tLoss: 1.365786\n",
      "Train Epoch: 52 [1432/2848 (50%)]\tLoss: 0.670777\n",
      "Train Epoch: 52 [1672/2848 (59%)]\tLoss: 0.473582\n",
      "Train Epoch: 52 [1912/2848 (67%)]\tLoss: 0.804852\n",
      "Train Epoch: 52 [2152/2848 (76%)]\tLoss: 1.677770\n",
      "Train Epoch: 52 [2392/2848 (84%)]\tLoss: 0.560110\n",
      "Train Epoch: 52 [2632/2848 (92%)]\tLoss: 0.657600\n",
      "\n",
      "Test set: Average loss: 0.3801, Accuracy: 144/200 (72%)\n",
      "\n",
      "Train Epoch: 53 [232/2848 (8%)]\tLoss: 0.723458\n",
      "Train Epoch: 53 [472/2848 (17%)]\tLoss: 0.626926\n",
      "Train Epoch: 53 [712/2848 (25%)]\tLoss: 1.243673\n",
      "Train Epoch: 53 [952/2848 (33%)]\tLoss: 0.822110\n",
      "Train Epoch: 53 [1192/2848 (42%)]\tLoss: 0.533924\n",
      "Train Epoch: 53 [1432/2848 (50%)]\tLoss: 1.027037\n",
      "Train Epoch: 53 [1672/2848 (59%)]\tLoss: 0.903083\n",
      "Train Epoch: 53 [1912/2848 (67%)]\tLoss: 0.768645\n",
      "Train Epoch: 53 [2152/2848 (76%)]\tLoss: 1.468038\n",
      "Train Epoch: 53 [2392/2848 (84%)]\tLoss: 0.929621\n",
      "Train Epoch: 53 [2632/2848 (92%)]\tLoss: 0.230588\n",
      "\n",
      "Test set: Average loss: 0.3506, Accuracy: 156/200 (78%)\n",
      "\n",
      "Train Epoch: 54 [232/2848 (8%)]\tLoss: 0.750120\n",
      "Train Epoch: 54 [472/2848 (17%)]\tLoss: 0.848653\n",
      "Train Epoch: 54 [712/2848 (25%)]\tLoss: 0.289444\n",
      "Train Epoch: 54 [952/2848 (33%)]\tLoss: 0.343298\n",
      "Train Epoch: 54 [1192/2848 (42%)]\tLoss: 0.877334\n",
      "Train Epoch: 54 [1432/2848 (50%)]\tLoss: 1.055377\n",
      "Train Epoch: 54 [1672/2848 (59%)]\tLoss: 0.575135\n",
      "Train Epoch: 54 [1912/2848 (67%)]\tLoss: 1.395484\n",
      "Train Epoch: 54 [2152/2848 (76%)]\tLoss: 0.847708\n",
      "Train Epoch: 54 [2392/2848 (84%)]\tLoss: 0.519070\n",
      "Train Epoch: 54 [2632/2848 (92%)]\tLoss: 0.604758\n",
      "\n",
      "Test set: Average loss: 0.4924, Accuracy: 143/200 (72%)\n",
      "\n",
      "Train Epoch: 55 [232/2848 (8%)]\tLoss: 0.413385\n",
      "Train Epoch: 55 [472/2848 (17%)]\tLoss: 1.051559\n",
      "Train Epoch: 55 [712/2848 (25%)]\tLoss: 0.427320\n",
      "Train Epoch: 55 [952/2848 (33%)]\tLoss: 0.926108\n",
      "Train Epoch: 55 [1192/2848 (42%)]\tLoss: 1.333829\n",
      "Train Epoch: 55 [1432/2848 (50%)]\tLoss: 0.636057\n",
      "Train Epoch: 55 [1672/2848 (59%)]\tLoss: 1.470275\n",
      "Train Epoch: 55 [1912/2848 (67%)]\tLoss: 0.719075\n",
      "Train Epoch: 55 [2152/2848 (76%)]\tLoss: 0.669843\n",
      "Train Epoch: 55 [2392/2848 (84%)]\tLoss: 0.939161\n",
      "Train Epoch: 55 [2632/2848 (92%)]\tLoss: 0.765157\n",
      "\n",
      "Test set: Average loss: 0.4019, Accuracy: 156/200 (78%)\n",
      "\n",
      "Train Epoch: 56 [232/2848 (8%)]\tLoss: 0.696279\n",
      "Train Epoch: 56 [472/2848 (17%)]\tLoss: 1.250569\n",
      "Train Epoch: 56 [712/2848 (25%)]\tLoss: 0.974563\n",
      "Train Epoch: 56 [952/2848 (33%)]\tLoss: 0.183702\n",
      "Train Epoch: 56 [1192/2848 (42%)]\tLoss: 1.345724\n",
      "Train Epoch: 56 [1432/2848 (50%)]\tLoss: 0.594274\n",
      "Train Epoch: 56 [1672/2848 (59%)]\tLoss: 0.984958\n",
      "Train Epoch: 56 [1912/2848 (67%)]\tLoss: 0.492837\n",
      "Train Epoch: 56 [2152/2848 (76%)]\tLoss: 1.234571\n",
      "Train Epoch: 56 [2392/2848 (84%)]\tLoss: 0.818984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [2632/2848 (92%)]\tLoss: 0.957933\n",
      "\n",
      "Test set: Average loss: 0.3815, Accuracy: 166/200 (83%)\n",
      "\n",
      "Train Epoch: 57 [232/2848 (8%)]\tLoss: 0.844749\n",
      "Train Epoch: 57 [472/2848 (17%)]\tLoss: 0.623440\n",
      "Train Epoch: 57 [712/2848 (25%)]\tLoss: 0.871789\n",
      "Train Epoch: 57 [952/2848 (33%)]\tLoss: 2.078491\n",
      "Train Epoch: 57 [1192/2848 (42%)]\tLoss: 1.245357\n",
      "Train Epoch: 57 [1432/2848 (50%)]\tLoss: 0.685629\n",
      "Train Epoch: 57 [1672/2848 (59%)]\tLoss: 1.450398\n",
      "Train Epoch: 57 [1912/2848 (67%)]\tLoss: 0.538629\n",
      "Train Epoch: 57 [2152/2848 (76%)]\tLoss: 1.009951\n",
      "Train Epoch: 57 [2392/2848 (84%)]\tLoss: 0.596832\n",
      "Train Epoch: 57 [2632/2848 (92%)]\tLoss: 0.567325\n",
      "\n",
      "Test set: Average loss: 0.3704, Accuracy: 177/200 (88%)\n",
      "\n",
      "Train Epoch: 58 [232/2848 (8%)]\tLoss: 1.507743\n",
      "Train Epoch: 58 [472/2848 (17%)]\tLoss: 0.578487\n",
      "Train Epoch: 58 [712/2848 (25%)]\tLoss: 0.610558\n",
      "Train Epoch: 58 [952/2848 (33%)]\tLoss: 0.904988\n",
      "Train Epoch: 58 [1192/2848 (42%)]\tLoss: 1.342466\n",
      "Train Epoch: 58 [1432/2848 (50%)]\tLoss: 0.549539\n",
      "Train Epoch: 58 [1672/2848 (59%)]\tLoss: 0.614832\n",
      "Train Epoch: 58 [1912/2848 (67%)]\tLoss: 0.338820\n",
      "Train Epoch: 58 [2152/2848 (76%)]\tLoss: 0.523900\n",
      "Train Epoch: 58 [2392/2848 (84%)]\tLoss: 1.001566\n",
      "Train Epoch: 58 [2632/2848 (92%)]\tLoss: 0.481111\n",
      "\n",
      "Test set: Average loss: 0.4440, Accuracy: 163/200 (82%)\n",
      "\n",
      "Train Epoch: 59 [232/2848 (8%)]\tLoss: 0.318900\n",
      "Train Epoch: 59 [472/2848 (17%)]\tLoss: 0.503589\n",
      "Train Epoch: 59 [712/2848 (25%)]\tLoss: 0.711579\n",
      "Train Epoch: 59 [952/2848 (33%)]\tLoss: 0.846319\n",
      "Train Epoch: 59 [1192/2848 (42%)]\tLoss: 1.657211\n",
      "Train Epoch: 59 [1432/2848 (50%)]\tLoss: 0.891934\n",
      "Train Epoch: 59 [1672/2848 (59%)]\tLoss: 1.321051\n",
      "Train Epoch: 59 [1912/2848 (67%)]\tLoss: 0.919177\n",
      "Train Epoch: 59 [2152/2848 (76%)]\tLoss: 0.675683\n",
      "Train Epoch: 59 [2392/2848 (84%)]\tLoss: 0.863850\n",
      "Train Epoch: 59 [2632/2848 (92%)]\tLoss: 0.566243\n",
      "\n",
      "Test set: Average loss: 0.4144, Accuracy: 169/200 (84%)\n",
      "\n",
      "Train Epoch: 60 [232/2848 (8%)]\tLoss: 1.300763\n",
      "Train Epoch: 60 [472/2848 (17%)]\tLoss: 0.854171\n",
      "Train Epoch: 60 [712/2848 (25%)]\tLoss: 0.606366\n",
      "Train Epoch: 60 [952/2848 (33%)]\tLoss: 0.476045\n",
      "Train Epoch: 60 [1192/2848 (42%)]\tLoss: 0.764631\n",
      "Train Epoch: 60 [1432/2848 (50%)]\tLoss: 0.621968\n",
      "Train Epoch: 60 [1672/2848 (59%)]\tLoss: 0.268685\n",
      "Train Epoch: 60 [1912/2848 (67%)]\tLoss: 0.843365\n",
      "Train Epoch: 60 [2152/2848 (76%)]\tLoss: 1.219394\n",
      "Train Epoch: 60 [2392/2848 (84%)]\tLoss: 0.492388\n",
      "Train Epoch: 60 [2632/2848 (92%)]\tLoss: 1.750406\n",
      "\n",
      "Test set: Average loss: 0.6639, Accuracy: 132/200 (66%)\n",
      "\n",
      "Train Epoch: 61 [232/2848 (8%)]\tLoss: 0.677338\n",
      "Train Epoch: 61 [472/2848 (17%)]\tLoss: 0.232211\n",
      "Train Epoch: 61 [712/2848 (25%)]\tLoss: 0.458925\n",
      "Train Epoch: 61 [952/2848 (33%)]\tLoss: 0.190707\n",
      "Train Epoch: 61 [1192/2848 (42%)]\tLoss: 0.611319\n",
      "Train Epoch: 61 [1432/2848 (50%)]\tLoss: 0.636923\n",
      "Train Epoch: 61 [1672/2848 (59%)]\tLoss: 0.817925\n",
      "Train Epoch: 61 [1912/2848 (67%)]\tLoss: 0.852272\n",
      "Train Epoch: 61 [2152/2848 (76%)]\tLoss: 0.579051\n",
      "Train Epoch: 61 [2392/2848 (84%)]\tLoss: 0.308374\n",
      "Train Epoch: 61 [2632/2848 (92%)]\tLoss: 0.697638\n",
      "\n",
      "Test set: Average loss: 0.5858, Accuracy: 167/200 (84%)\n",
      "\n",
      "Train Epoch: 62 [232/2848 (8%)]\tLoss: 0.385938\n",
      "Train Epoch: 62 [472/2848 (17%)]\tLoss: 0.463414\n",
      "Train Epoch: 62 [712/2848 (25%)]\tLoss: 1.332745\n",
      "Train Epoch: 62 [952/2848 (33%)]\tLoss: 1.168981\n",
      "Train Epoch: 62 [1192/2848 (42%)]\tLoss: 0.184919\n",
      "Train Epoch: 62 [1432/2848 (50%)]\tLoss: 0.160213\n",
      "Train Epoch: 62 [1672/2848 (59%)]\tLoss: 1.050259\n",
      "Train Epoch: 62 [1912/2848 (67%)]\tLoss: 0.949765\n",
      "Train Epoch: 62 [2152/2848 (76%)]\tLoss: 0.798905\n",
      "Train Epoch: 62 [2392/2848 (84%)]\tLoss: 0.724150\n",
      "Train Epoch: 62 [2632/2848 (92%)]\tLoss: 0.630673\n",
      "\n",
      "Test set: Average loss: 0.5216, Accuracy: 152/200 (76%)\n",
      "\n",
      "Train Epoch: 63 [232/2848 (8%)]\tLoss: 0.289818\n",
      "Train Epoch: 63 [472/2848 (17%)]\tLoss: 0.641756\n",
      "Train Epoch: 63 [712/2848 (25%)]\tLoss: 1.142139\n",
      "Train Epoch: 63 [952/2848 (33%)]\tLoss: 0.682754\n",
      "Train Epoch: 63 [1192/2848 (42%)]\tLoss: 0.486558\n",
      "Train Epoch: 63 [1432/2848 (50%)]\tLoss: 0.865034\n",
      "Train Epoch: 63 [1672/2848 (59%)]\tLoss: 1.134588\n",
      "Train Epoch: 63 [1912/2848 (67%)]\tLoss: 0.310883\n",
      "Train Epoch: 63 [2152/2848 (76%)]\tLoss: 0.964352\n",
      "Train Epoch: 63 [2392/2848 (84%)]\tLoss: 0.532002\n",
      "Train Epoch: 63 [2632/2848 (92%)]\tLoss: 0.702749\n",
      "\n",
      "Test set: Average loss: 0.5061, Accuracy: 179/200 (90%)\n",
      "\n",
      "Train Epoch: 64 [232/2848 (8%)]\tLoss: 1.233658\n",
      "Train Epoch: 64 [472/2848 (17%)]\tLoss: 0.636040\n",
      "Train Epoch: 64 [712/2848 (25%)]\tLoss: 0.840555\n",
      "Train Epoch: 64 [952/2848 (33%)]\tLoss: 0.483950\n",
      "Train Epoch: 64 [1192/2848 (42%)]\tLoss: 0.315475\n",
      "Train Epoch: 64 [1432/2848 (50%)]\tLoss: 0.844749\n",
      "Train Epoch: 64 [1672/2848 (59%)]\tLoss: 0.230646\n",
      "Train Epoch: 64 [1912/2848 (67%)]\tLoss: 0.169768\n",
      "Train Epoch: 64 [2152/2848 (76%)]\tLoss: 0.359531\n",
      "Train Epoch: 64 [2392/2848 (84%)]\tLoss: 1.217648\n",
      "Train Epoch: 64 [2632/2848 (92%)]\tLoss: 2.161005\n",
      "\n",
      "Test set: Average loss: 0.4127, Accuracy: 171/200 (86%)\n",
      "\n",
      "Train Epoch: 65 [232/2848 (8%)]\tLoss: 0.517102\n",
      "Train Epoch: 65 [472/2848 (17%)]\tLoss: 0.109971\n",
      "Train Epoch: 65 [712/2848 (25%)]\tLoss: 0.311967\n",
      "Train Epoch: 65 [952/2848 (33%)]\tLoss: 0.501942\n",
      "Train Epoch: 65 [1192/2848 (42%)]\tLoss: 0.318789\n",
      "Train Epoch: 65 [1432/2848 (50%)]\tLoss: 0.333326\n",
      "Train Epoch: 65 [1672/2848 (59%)]\tLoss: 0.465039\n",
      "Train Epoch: 65 [1912/2848 (67%)]\tLoss: 0.615385\n",
      "Train Epoch: 65 [2152/2848 (76%)]\tLoss: 0.656792\n",
      "Train Epoch: 65 [2392/2848 (84%)]\tLoss: 0.992820\n",
      "Train Epoch: 65 [2632/2848 (92%)]\tLoss: 0.670047\n",
      "\n",
      "Test set: Average loss: 0.3103, Accuracy: 183/200 (92%)\n",
      "\n",
      "Train Epoch: 66 [232/2848 (8%)]\tLoss: 0.942128\n",
      "Train Epoch: 66 [472/2848 (17%)]\tLoss: 0.434331\n",
      "Train Epoch: 66 [712/2848 (25%)]\tLoss: 0.866446\n",
      "Train Epoch: 66 [952/2848 (33%)]\tLoss: 0.700773\n",
      "Train Epoch: 66 [1192/2848 (42%)]\tLoss: 0.518169\n",
      "Train Epoch: 66 [1432/2848 (50%)]\tLoss: 0.670085\n",
      "Train Epoch: 66 [1672/2848 (59%)]\tLoss: 0.273829\n",
      "Train Epoch: 66 [1912/2848 (67%)]\tLoss: 0.325824\n",
      "Train Epoch: 66 [2152/2848 (76%)]\tLoss: 1.023064\n",
      "Train Epoch: 66 [2392/2848 (84%)]\tLoss: 0.332111\n",
      "Train Epoch: 66 [2632/2848 (92%)]\tLoss: 0.807400\n",
      "\n",
      "Test set: Average loss: 0.3489, Accuracy: 187/200 (94%)\n",
      "\n",
      "Train Epoch: 67 [232/2848 (8%)]\tLoss: 0.384995\n",
      "Train Epoch: 67 [472/2848 (17%)]\tLoss: 0.299816\n",
      "Train Epoch: 67 [712/2848 (25%)]\tLoss: 0.631510\n",
      "Train Epoch: 67 [952/2848 (33%)]\tLoss: 0.591194\n",
      "Train Epoch: 67 [1192/2848 (42%)]\tLoss: 0.436592\n",
      "Train Epoch: 67 [1432/2848 (50%)]\tLoss: 0.222105\n",
      "Train Epoch: 67 [1672/2848 (59%)]\tLoss: 0.121303\n",
      "Train Epoch: 67 [1912/2848 (67%)]\tLoss: 0.601642\n",
      "Train Epoch: 67 [2152/2848 (76%)]\tLoss: 0.788082\n",
      "Train Epoch: 67 [2392/2848 (84%)]\tLoss: 0.626393\n",
      "Train Epoch: 67 [2632/2848 (92%)]\tLoss: 0.222545\n",
      "\n",
      "Test set: Average loss: 0.6901, Accuracy: 119/200 (60%)\n",
      "\n",
      "Train Epoch: 68 [232/2848 (8%)]\tLoss: 0.597524\n",
      "Train Epoch: 68 [472/2848 (17%)]\tLoss: 0.236667\n",
      "Train Epoch: 68 [712/2848 (25%)]\tLoss: 0.220774\n",
      "Train Epoch: 68 [952/2848 (33%)]\tLoss: 0.641392\n",
      "Train Epoch: 68 [1192/2848 (42%)]\tLoss: 0.463237\n",
      "Train Epoch: 68 [1432/2848 (50%)]\tLoss: 0.541413\n",
      "Train Epoch: 68 [1672/2848 (59%)]\tLoss: 0.301084\n",
      "Train Epoch: 68 [1912/2848 (67%)]\tLoss: 0.215013\n",
      "Train Epoch: 68 [2152/2848 (76%)]\tLoss: 0.182794\n",
      "Train Epoch: 68 [2392/2848 (84%)]\tLoss: 0.667928\n",
      "Train Epoch: 68 [2632/2848 (92%)]\tLoss: 0.054602\n",
      "\n",
      "Test set: Average loss: 0.3766, Accuracy: 197/200 (98%)\n",
      "\n",
      "Train Epoch: 69 [232/2848 (8%)]\tLoss: 0.108991\n",
      "Train Epoch: 69 [472/2848 (17%)]\tLoss: 0.189259\n",
      "Train Epoch: 69 [712/2848 (25%)]\tLoss: 0.504239\n",
      "Train Epoch: 69 [952/2848 (33%)]\tLoss: 0.242340\n",
      "Train Epoch: 69 [1192/2848 (42%)]\tLoss: 0.139775\n",
      "Train Epoch: 69 [1432/2848 (50%)]\tLoss: 0.464005\n",
      "Train Epoch: 69 [1672/2848 (59%)]\tLoss: 0.275737\n",
      "Train Epoch: 69 [1912/2848 (67%)]\tLoss: 0.842847\n",
      "Train Epoch: 69 [2152/2848 (76%)]\tLoss: 0.285701\n",
      "Train Epoch: 69 [2392/2848 (84%)]\tLoss: 0.395657\n",
      "Train Epoch: 69 [2632/2848 (92%)]\tLoss: 0.140348\n",
      "\n",
      "Test set: Average loss: 0.3377, Accuracy: 198/200 (99%)\n",
      "\n",
      "Train Epoch: 70 [232/2848 (8%)]\tLoss: 0.014403\n",
      "Train Epoch: 70 [472/2848 (17%)]\tLoss: 0.454086\n",
      "Train Epoch: 70 [712/2848 (25%)]\tLoss: 0.457801\n",
      "Train Epoch: 70 [952/2848 (33%)]\tLoss: 0.207578\n",
      "Train Epoch: 70 [1192/2848 (42%)]\tLoss: 0.150648\n",
      "Train Epoch: 70 [1432/2848 (50%)]\tLoss: 0.466235\n",
      "Train Epoch: 70 [1672/2848 (59%)]\tLoss: 0.254581\n",
      "Train Epoch: 70 [1912/2848 (67%)]\tLoss: 0.302254\n",
      "Train Epoch: 70 [2152/2848 (76%)]\tLoss: 1.026780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 70 [2392/2848 (84%)]\tLoss: 0.562734\n",
      "Train Epoch: 70 [2632/2848 (92%)]\tLoss: 0.887148\n",
      "\n",
      "Test set: Average loss: 0.4867, Accuracy: 174/200 (87%)\n",
      "\n",
      "Train Epoch: 71 [232/2848 (8%)]\tLoss: 0.398502\n",
      "Train Epoch: 71 [472/2848 (17%)]\tLoss: 0.115658\n",
      "Train Epoch: 71 [712/2848 (25%)]\tLoss: 0.249168\n",
      "Train Epoch: 71 [952/2848 (33%)]\tLoss: 0.514450\n",
      "Train Epoch: 71 [1192/2848 (42%)]\tLoss: 1.275263\n",
      "Train Epoch: 71 [1432/2848 (50%)]\tLoss: 0.849116\n",
      "Train Epoch: 71 [1672/2848 (59%)]\tLoss: 0.432839\n",
      "Train Epoch: 71 [1912/2848 (67%)]\tLoss: 0.267925\n",
      "Train Epoch: 71 [2152/2848 (76%)]\tLoss: 0.121595\n",
      "Train Epoch: 71 [2392/2848 (84%)]\tLoss: 0.378206\n",
      "Train Epoch: 71 [2632/2848 (92%)]\tLoss: 0.090384\n",
      "\n",
      "Test set: Average loss: 0.3790, Accuracy: 193/200 (96%)\n",
      "\n",
      "Train Epoch: 72 [232/2848 (8%)]\tLoss: 0.257447\n",
      "Train Epoch: 72 [472/2848 (17%)]\tLoss: 0.576310\n",
      "Train Epoch: 72 [712/2848 (25%)]\tLoss: 1.488587\n",
      "Train Epoch: 72 [952/2848 (33%)]\tLoss: 0.837183\n",
      "Train Epoch: 72 [1192/2848 (42%)]\tLoss: 0.283509\n",
      "Train Epoch: 72 [1432/2848 (50%)]\tLoss: 0.143216\n",
      "Train Epoch: 72 [1672/2848 (59%)]\tLoss: 0.369141\n",
      "Train Epoch: 72 [1912/2848 (67%)]\tLoss: 0.495875\n",
      "Train Epoch: 72 [2152/2848 (76%)]\tLoss: 0.331291\n",
      "Train Epoch: 72 [2392/2848 (84%)]\tLoss: 0.170925\n",
      "Train Epoch: 72 [2632/2848 (92%)]\tLoss: 0.224136\n",
      "\n",
      "Test set: Average loss: 0.3665, Accuracy: 195/200 (98%)\n",
      "\n",
      "Train Epoch: 73 [232/2848 (8%)]\tLoss: 0.224139\n",
      "Train Epoch: 73 [472/2848 (17%)]\tLoss: 0.792107\n",
      "Train Epoch: 73 [712/2848 (25%)]\tLoss: 0.305640\n",
      "Train Epoch: 73 [952/2848 (33%)]\tLoss: 0.726689\n",
      "Train Epoch: 73 [1192/2848 (42%)]\tLoss: 0.432989\n",
      "Train Epoch: 73 [1432/2848 (50%)]\tLoss: 0.580778\n",
      "Train Epoch: 73 [1672/2848 (59%)]\tLoss: 0.214110\n",
      "Train Epoch: 73 [1912/2848 (67%)]\tLoss: 0.094986\n",
      "Train Epoch: 73 [2152/2848 (76%)]\tLoss: 0.276435\n",
      "Train Epoch: 73 [2392/2848 (84%)]\tLoss: 1.149134\n",
      "Train Epoch: 73 [2632/2848 (92%)]\tLoss: 0.617988\n",
      "\n",
      "Test set: Average loss: 0.3537, Accuracy: 203/200 (102%)\n",
      "\n",
      "Train Epoch: 74 [232/2848 (8%)]\tLoss: 0.321966\n",
      "Train Epoch: 74 [472/2848 (17%)]\tLoss: 0.330740\n",
      "Train Epoch: 74 [712/2848 (25%)]\tLoss: 0.251678\n",
      "Train Epoch: 74 [952/2848 (33%)]\tLoss: 0.877281\n",
      "Train Epoch: 74 [1192/2848 (42%)]\tLoss: 0.435178\n",
      "Train Epoch: 74 [1432/2848 (50%)]\tLoss: 0.901542\n",
      "Train Epoch: 74 [1672/2848 (59%)]\tLoss: 0.217215\n",
      "Train Epoch: 74 [1912/2848 (67%)]\tLoss: 0.200280\n",
      "Train Epoch: 74 [2152/2848 (76%)]\tLoss: 1.071095\n",
      "Train Epoch: 74 [2392/2848 (84%)]\tLoss: 0.208483\n",
      "Train Epoch: 74 [2632/2848 (92%)]\tLoss: 0.248509\n",
      "\n",
      "Test set: Average loss: 0.5206, Accuracy: 176/200 (88%)\n",
      "\n",
      "Train Epoch: 75 [232/2848 (8%)]\tLoss: 0.433760\n",
      "Train Epoch: 75 [472/2848 (17%)]\tLoss: 0.713476\n",
      "Train Epoch: 75 [712/2848 (25%)]\tLoss: 0.058363\n",
      "Train Epoch: 75 [952/2848 (33%)]\tLoss: 0.463944\n",
      "Train Epoch: 75 [1192/2848 (42%)]\tLoss: 0.194831\n",
      "Train Epoch: 75 [1432/2848 (50%)]\tLoss: 0.143150\n",
      "Train Epoch: 75 [1672/2848 (59%)]\tLoss: 0.820205\n",
      "Train Epoch: 75 [1912/2848 (67%)]\tLoss: 0.453905\n",
      "Train Epoch: 75 [2152/2848 (76%)]\tLoss: 1.265405\n",
      "Train Epoch: 75 [2392/2848 (84%)]\tLoss: 0.153071\n",
      "Train Epoch: 75 [2632/2848 (92%)]\tLoss: 0.863011\n",
      "\n",
      "Test set: Average loss: 0.4970, Accuracy: 182/200 (91%)\n",
      "\n",
      "Train Epoch: 76 [232/2848 (8%)]\tLoss: 0.197634\n",
      "Train Epoch: 76 [472/2848 (17%)]\tLoss: 0.799607\n",
      "Train Epoch: 76 [712/2848 (25%)]\tLoss: 0.082950\n",
      "Train Epoch: 76 [952/2848 (33%)]\tLoss: 0.827221\n",
      "Train Epoch: 76 [1192/2848 (42%)]\tLoss: 0.444222\n",
      "Train Epoch: 76 [1432/2848 (50%)]\tLoss: 1.480979\n",
      "Train Epoch: 76 [1672/2848 (59%)]\tLoss: 0.613318\n",
      "Train Epoch: 76 [1912/2848 (67%)]\tLoss: 0.392847\n",
      "Train Epoch: 76 [2152/2848 (76%)]\tLoss: 0.168077\n",
      "Train Epoch: 76 [2392/2848 (84%)]\tLoss: 0.670971\n",
      "Train Epoch: 76 [2632/2848 (92%)]\tLoss: 1.341043\n",
      "\n",
      "Test set: Average loss: 0.3896, Accuracy: 171/200 (86%)\n",
      "\n",
      "Train Epoch: 77 [232/2848 (8%)]\tLoss: 0.255593\n",
      "Train Epoch: 77 [472/2848 (17%)]\tLoss: 0.587544\n",
      "Train Epoch: 77 [712/2848 (25%)]\tLoss: 0.090736\n",
      "Train Epoch: 77 [952/2848 (33%)]\tLoss: 0.204530\n",
      "Train Epoch: 77 [1192/2848 (42%)]\tLoss: 0.730542\n",
      "Train Epoch: 77 [1432/2848 (50%)]\tLoss: 0.359394\n",
      "Train Epoch: 77 [1672/2848 (59%)]\tLoss: 0.119224\n",
      "Train Epoch: 77 [1912/2848 (67%)]\tLoss: 0.349330\n",
      "Train Epoch: 77 [2152/2848 (76%)]\tLoss: 0.195585\n",
      "Train Epoch: 77 [2392/2848 (84%)]\tLoss: 0.406210\n",
      "Train Epoch: 77 [2632/2848 (92%)]\tLoss: 0.354490\n",
      "\n",
      "Test set: Average loss: 0.3522, Accuracy: 199/200 (100%)\n",
      "\n",
      "Train Epoch: 78 [232/2848 (8%)]\tLoss: 0.283377\n",
      "Train Epoch: 78 [472/2848 (17%)]\tLoss: 0.572823\n",
      "Train Epoch: 78 [712/2848 (25%)]\tLoss: 0.006136\n",
      "Train Epoch: 78 [952/2848 (33%)]\tLoss: 0.291779\n",
      "Train Epoch: 78 [1192/2848 (42%)]\tLoss: 0.569100\n",
      "Train Epoch: 78 [1432/2848 (50%)]\tLoss: 0.165483\n",
      "Train Epoch: 78 [1672/2848 (59%)]\tLoss: 0.794525\n",
      "Train Epoch: 78 [1912/2848 (67%)]\tLoss: 0.959452\n",
      "Train Epoch: 78 [2152/2848 (76%)]\tLoss: 0.523428\n",
      "Train Epoch: 78 [2392/2848 (84%)]\tLoss: 0.207637\n",
      "Train Epoch: 78 [2632/2848 (92%)]\tLoss: 0.137529\n",
      "\n",
      "Test set: Average loss: 0.3146, Accuracy: 213/200 (106%)\n",
      "\n",
      "Train Epoch: 79 [232/2848 (8%)]\tLoss: 0.643748\n",
      "Train Epoch: 79 [472/2848 (17%)]\tLoss: 0.212179\n",
      "Train Epoch: 79 [712/2848 (25%)]\tLoss: 1.199665\n",
      "Train Epoch: 79 [952/2848 (33%)]\tLoss: 1.131738\n",
      "Train Epoch: 79 [1192/2848 (42%)]\tLoss: 0.886163\n",
      "Train Epoch: 79 [1432/2848 (50%)]\tLoss: 0.424927\n",
      "Train Epoch: 79 [1672/2848 (59%)]\tLoss: 0.096829\n",
      "Train Epoch: 79 [1912/2848 (67%)]\tLoss: 0.138700\n",
      "Train Epoch: 79 [2152/2848 (76%)]\tLoss: 0.273132\n",
      "Train Epoch: 79 [2392/2848 (84%)]\tLoss: 0.198993\n",
      "Train Epoch: 79 [2632/2848 (92%)]\tLoss: 0.025209\n",
      "\n",
      "Test set: Average loss: 0.3405, Accuracy: 215/200 (108%)\n",
      "\n",
      "Train Epoch: 80 [232/2848 (8%)]\tLoss: 0.306650\n",
      "Train Epoch: 80 [472/2848 (17%)]\tLoss: 0.276286\n",
      "Train Epoch: 80 [712/2848 (25%)]\tLoss: 0.358143\n",
      "Train Epoch: 80 [952/2848 (33%)]\tLoss: 0.414194\n",
      "Train Epoch: 80 [1192/2848 (42%)]\tLoss: 0.713927\n",
      "Train Epoch: 80 [1432/2848 (50%)]\tLoss: 0.070615\n",
      "Train Epoch: 80 [1672/2848 (59%)]\tLoss: 0.717912\n",
      "Train Epoch: 80 [1912/2848 (67%)]\tLoss: 0.316303\n",
      "Train Epoch: 80 [2152/2848 (76%)]\tLoss: 0.020195\n",
      "Train Epoch: 80 [2392/2848 (84%)]\tLoss: 0.051646\n",
      "Train Epoch: 80 [2632/2848 (92%)]\tLoss: 0.491459\n",
      "\n",
      "Test set: Average loss: 0.5369, Accuracy: 194/200 (97%)\n",
      "\n",
      "Train Epoch: 81 [232/2848 (8%)]\tLoss: 0.173832\n",
      "Train Epoch: 81 [472/2848 (17%)]\tLoss: 0.275661\n",
      "Train Epoch: 81 [712/2848 (25%)]\tLoss: 0.875567\n",
      "Train Epoch: 81 [952/2848 (33%)]\tLoss: 0.049734\n",
      "Train Epoch: 81 [1192/2848 (42%)]\tLoss: 0.313442\n",
      "Train Epoch: 81 [1432/2848 (50%)]\tLoss: 0.191648\n",
      "Train Epoch: 81 [1672/2848 (59%)]\tLoss: 0.292649\n",
      "Train Epoch: 81 [1912/2848 (67%)]\tLoss: 0.975107\n",
      "Train Epoch: 81 [2152/2848 (76%)]\tLoss: 0.616423\n",
      "Train Epoch: 81 [2392/2848 (84%)]\tLoss: 0.102618\n",
      "Train Epoch: 81 [2632/2848 (92%)]\tLoss: 0.413312\n",
      "\n",
      "Test set: Average loss: 0.5070, Accuracy: 196/200 (98%)\n",
      "\n",
      "Train Epoch: 82 [232/2848 (8%)]\tLoss: 0.158613\n",
      "Train Epoch: 82 [472/2848 (17%)]\tLoss: 0.595638\n",
      "Train Epoch: 82 [712/2848 (25%)]\tLoss: 0.832736\n",
      "Train Epoch: 82 [952/2848 (33%)]\tLoss: 0.131027\n",
      "Train Epoch: 82 [1192/2848 (42%)]\tLoss: 0.142735\n",
      "Train Epoch: 82 [1432/2848 (50%)]\tLoss: 0.116533\n",
      "Train Epoch: 82 [1672/2848 (59%)]\tLoss: 0.125482\n",
      "Train Epoch: 82 [1912/2848 (67%)]\tLoss: 0.196707\n",
      "Train Epoch: 82 [2152/2848 (76%)]\tLoss: 0.444762\n",
      "Train Epoch: 82 [2392/2848 (84%)]\tLoss: 0.518128\n",
      "Train Epoch: 82 [2632/2848 (92%)]\tLoss: 0.265943\n",
      "\n",
      "Test set: Average loss: 0.3769, Accuracy: 198/200 (99%)\n",
      "\n",
      "Train Epoch: 83 [232/2848 (8%)]\tLoss: 0.914968\n",
      "Train Epoch: 83 [472/2848 (17%)]\tLoss: 0.309284\n",
      "Train Epoch: 83 [712/2848 (25%)]\tLoss: 0.390308\n",
      "Train Epoch: 83 [952/2848 (33%)]\tLoss: 0.352115\n",
      "Train Epoch: 83 [1192/2848 (42%)]\tLoss: 0.780480\n",
      "Train Epoch: 83 [1432/2848 (50%)]\tLoss: 0.900793\n",
      "Train Epoch: 83 [1672/2848 (59%)]\tLoss: 0.079733\n",
      "Train Epoch: 83 [1912/2848 (67%)]\tLoss: 0.492372\n",
      "Train Epoch: 83 [2152/2848 (76%)]\tLoss: 0.493343\n",
      "Train Epoch: 83 [2392/2848 (84%)]\tLoss: 0.281692\n",
      "Train Epoch: 83 [2632/2848 (92%)]\tLoss: 0.397388\n",
      "\n",
      "Test set: Average loss: 0.3017, Accuracy: 221/200 (110%)\n",
      "\n",
      "Train Epoch: 84 [232/2848 (8%)]\tLoss: 0.379141\n",
      "Train Epoch: 84 [472/2848 (17%)]\tLoss: 0.143554\n",
      "Train Epoch: 84 [712/2848 (25%)]\tLoss: 0.541891\n",
      "Train Epoch: 84 [952/2848 (33%)]\tLoss: 0.306542\n",
      "Train Epoch: 84 [1192/2848 (42%)]\tLoss: 0.338047\n",
      "Train Epoch: 84 [1432/2848 (50%)]\tLoss: 0.078189\n",
      "Train Epoch: 84 [1672/2848 (59%)]\tLoss: 0.176015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 84 [1912/2848 (67%)]\tLoss: 0.609021\n",
      "Train Epoch: 84 [2152/2848 (76%)]\tLoss: 0.289151\n",
      "Train Epoch: 84 [2392/2848 (84%)]\tLoss: 0.330248\n",
      "Train Epoch: 84 [2632/2848 (92%)]\tLoss: 0.262304\n",
      "\n",
      "Test set: Average loss: 0.3199, Accuracy: 229/200 (114%)\n",
      "\n",
      "Train Epoch: 85 [232/2848 (8%)]\tLoss: 0.499249\n",
      "Train Epoch: 85 [472/2848 (17%)]\tLoss: 0.814739\n",
      "Train Epoch: 85 [712/2848 (25%)]\tLoss: 0.282914\n",
      "Train Epoch: 85 [952/2848 (33%)]\tLoss: 0.246319\n",
      "Train Epoch: 85 [1192/2848 (42%)]\tLoss: 0.490537\n",
      "Train Epoch: 85 [1432/2848 (50%)]\tLoss: 0.767853\n",
      "Train Epoch: 85 [1672/2848 (59%)]\tLoss: 0.235877\n",
      "Train Epoch: 85 [1912/2848 (67%)]\tLoss: 0.121014\n",
      "Train Epoch: 85 [2152/2848 (76%)]\tLoss: 0.179683\n",
      "Train Epoch: 85 [2392/2848 (84%)]\tLoss: 0.288470\n",
      "Train Epoch: 85 [2632/2848 (92%)]\tLoss: 0.318012\n",
      "\n",
      "Test set: Average loss: 0.3840, Accuracy: 204/200 (102%)\n",
      "\n",
      "Train Epoch: 86 [232/2848 (8%)]\tLoss: 0.335878\n",
      "Train Epoch: 86 [472/2848 (17%)]\tLoss: 0.101009\n",
      "Train Epoch: 86 [712/2848 (25%)]\tLoss: 0.407774\n",
      "Train Epoch: 86 [952/2848 (33%)]\tLoss: 0.506276\n",
      "Train Epoch: 86 [1192/2848 (42%)]\tLoss: 0.219236\n",
      "Train Epoch: 86 [1432/2848 (50%)]\tLoss: 0.279814\n",
      "Train Epoch: 86 [1672/2848 (59%)]\tLoss: 0.578200\n",
      "Train Epoch: 86 [1912/2848 (67%)]\tLoss: 0.516078\n",
      "Train Epoch: 86 [2152/2848 (76%)]\tLoss: 0.473778\n",
      "Train Epoch: 86 [2392/2848 (84%)]\tLoss: 0.266165\n",
      "Train Epoch: 86 [2632/2848 (92%)]\tLoss: 0.122408\n",
      "\n",
      "Test set: Average loss: 0.3058, Accuracy: 220/200 (110%)\n",
      "\n",
      "Train Epoch: 87 [232/2848 (8%)]\tLoss: 0.432147\n",
      "Train Epoch: 87 [472/2848 (17%)]\tLoss: 0.238440\n",
      "Train Epoch: 87 [712/2848 (25%)]\tLoss: 0.036933\n",
      "Train Epoch: 87 [952/2848 (33%)]\tLoss: 0.494294\n",
      "Train Epoch: 87 [1192/2848 (42%)]\tLoss: 0.037849\n",
      "Train Epoch: 87 [1432/2848 (50%)]\tLoss: 0.334243\n",
      "Train Epoch: 87 [1672/2848 (59%)]\tLoss: 0.146136\n",
      "Train Epoch: 87 [1912/2848 (67%)]\tLoss: 0.481820\n",
      "Train Epoch: 87 [2152/2848 (76%)]\tLoss: 0.296230\n",
      "Train Epoch: 87 [2392/2848 (84%)]\tLoss: 0.093093\n",
      "Train Epoch: 87 [2632/2848 (92%)]\tLoss: 0.128611\n",
      "\n",
      "Test set: Average loss: 0.3958, Accuracy: 200/200 (100%)\n",
      "\n",
      "Train Epoch: 88 [232/2848 (8%)]\tLoss: 0.049031\n",
      "Train Epoch: 88 [472/2848 (17%)]\tLoss: 0.555425\n",
      "Train Epoch: 88 [712/2848 (25%)]\tLoss: 0.239355\n",
      "Train Epoch: 88 [952/2848 (33%)]\tLoss: 0.110711\n",
      "Train Epoch: 88 [1192/2848 (42%)]\tLoss: 0.074105\n",
      "Train Epoch: 88 [1432/2848 (50%)]\tLoss: 0.149695\n",
      "Train Epoch: 88 [1672/2848 (59%)]\tLoss: 0.086767\n",
      "Train Epoch: 88 [1912/2848 (67%)]\tLoss: 0.482421\n",
      "Train Epoch: 88 [2152/2848 (76%)]\tLoss: 0.822550\n",
      "Train Epoch: 88 [2392/2848 (84%)]\tLoss: 0.127225\n",
      "Train Epoch: 88 [2632/2848 (92%)]\tLoss: 0.040108\n",
      "\n",
      "Test set: Average loss: 0.3596, Accuracy: 212/200 (106%)\n",
      "\n",
      "Train Epoch: 89 [232/2848 (8%)]\tLoss: 0.045697\n",
      "Train Epoch: 89 [472/2848 (17%)]\tLoss: 0.392295\n",
      "Train Epoch: 89 [712/2848 (25%)]\tLoss: 0.026271\n",
      "Train Epoch: 89 [952/2848 (33%)]\tLoss: 0.370269\n",
      "Train Epoch: 89 [1192/2848 (42%)]\tLoss: 0.671212\n",
      "Train Epoch: 89 [1432/2848 (50%)]\tLoss: 0.068062\n",
      "Train Epoch: 89 [1672/2848 (59%)]\tLoss: 0.232451\n",
      "Train Epoch: 89 [1912/2848 (67%)]\tLoss: 0.879454\n",
      "Train Epoch: 89 [2152/2848 (76%)]\tLoss: 0.087091\n",
      "Train Epoch: 89 [2392/2848 (84%)]\tLoss: 0.672194\n",
      "Train Epoch: 89 [2632/2848 (92%)]\tLoss: 0.315198\n",
      "\n",
      "Test set: Average loss: 0.3310, Accuracy: 202/200 (101%)\n",
      "\n",
      "Train Epoch: 90 [232/2848 (8%)]\tLoss: 0.042565\n",
      "Train Epoch: 90 [472/2848 (17%)]\tLoss: 0.245777\n",
      "Train Epoch: 90 [712/2848 (25%)]\tLoss: 0.836575\n",
      "Train Epoch: 90 [952/2848 (33%)]\tLoss: 0.110344\n",
      "Train Epoch: 90 [1192/2848 (42%)]\tLoss: 0.160560\n",
      "Train Epoch: 90 [1432/2848 (50%)]\tLoss: 0.639761\n",
      "Train Epoch: 90 [1672/2848 (59%)]\tLoss: 0.272058\n",
      "Train Epoch: 90 [1912/2848 (67%)]\tLoss: 0.351257\n",
      "Train Epoch: 90 [2152/2848 (76%)]\tLoss: 0.171972\n",
      "Train Epoch: 90 [2392/2848 (84%)]\tLoss: 0.190427\n",
      "Train Epoch: 90 [2632/2848 (92%)]\tLoss: 0.202980\n",
      "\n",
      "Test set: Average loss: 0.4650, Accuracy: 196/200 (98%)\n",
      "\n",
      "Train Epoch: 91 [232/2848 (8%)]\tLoss: 0.565716\n",
      "Train Epoch: 91 [472/2848 (17%)]\tLoss: 0.658767\n",
      "Train Epoch: 91 [712/2848 (25%)]\tLoss: 0.210280\n",
      "Train Epoch: 91 [952/2848 (33%)]\tLoss: 0.055237\n",
      "Train Epoch: 91 [1192/2848 (42%)]\tLoss: 0.135292\n",
      "Train Epoch: 91 [1432/2848 (50%)]\tLoss: 0.049454\n",
      "Train Epoch: 91 [1672/2848 (59%)]\tLoss: 0.140373\n",
      "Train Epoch: 91 [1912/2848 (67%)]\tLoss: 0.275218\n",
      "Train Epoch: 91 [2152/2848 (76%)]\tLoss: 1.033746\n",
      "Train Epoch: 91 [2392/2848 (84%)]\tLoss: 0.165302\n",
      "Train Epoch: 91 [2632/2848 (92%)]\tLoss: 0.478862\n",
      "\n",
      "Test set: Average loss: 0.3289, Accuracy: 210/200 (105%)\n",
      "\n",
      "Train Epoch: 92 [232/2848 (8%)]\tLoss: 0.100960\n",
      "Train Epoch: 92 [472/2848 (17%)]\tLoss: 0.206618\n",
      "Train Epoch: 92 [712/2848 (25%)]\tLoss: 0.134001\n",
      "Train Epoch: 92 [952/2848 (33%)]\tLoss: 0.332508\n",
      "Train Epoch: 92 [1192/2848 (42%)]\tLoss: 0.071731\n",
      "Train Epoch: 92 [1432/2848 (50%)]\tLoss: 0.071239\n",
      "Train Epoch: 92 [1672/2848 (59%)]\tLoss: 0.376956\n",
      "Train Epoch: 92 [1912/2848 (67%)]\tLoss: 0.070968\n",
      "Train Epoch: 92 [2152/2848 (76%)]\tLoss: 0.397976\n",
      "Train Epoch: 92 [2392/2848 (84%)]\tLoss: 0.326499\n",
      "Train Epoch: 92 [2632/2848 (92%)]\tLoss: 0.144586\n",
      "\n",
      "Test set: Average loss: 0.3162, Accuracy: 221/200 (110%)\n",
      "\n",
      "Train Epoch: 93 [232/2848 (8%)]\tLoss: 0.632568\n",
      "Train Epoch: 93 [472/2848 (17%)]\tLoss: 0.470315\n",
      "Train Epoch: 93 [712/2848 (25%)]\tLoss: 0.029666\n",
      "Train Epoch: 93 [952/2848 (33%)]\tLoss: 0.385579\n",
      "Train Epoch: 93 [1192/2848 (42%)]\tLoss: 0.411524\n",
      "Train Epoch: 93 [1432/2848 (50%)]\tLoss: 0.148433\n",
      "Train Epoch: 93 [1672/2848 (59%)]\tLoss: 0.386394\n",
      "Train Epoch: 93 [1912/2848 (67%)]\tLoss: 0.605286\n",
      "Train Epoch: 93 [2152/2848 (76%)]\tLoss: 0.380214\n",
      "Train Epoch: 93 [2392/2848 (84%)]\tLoss: 0.229552\n",
      "Train Epoch: 93 [2632/2848 (92%)]\tLoss: 0.448150\n",
      "\n",
      "Test set: Average loss: 0.3420, Accuracy: 222/200 (111%)\n",
      "\n",
      "Train Epoch: 94 [232/2848 (8%)]\tLoss: 0.046330\n",
      "Train Epoch: 94 [472/2848 (17%)]\tLoss: 0.458926\n",
      "Train Epoch: 94 [712/2848 (25%)]\tLoss: 0.095031\n",
      "Train Epoch: 94 [952/2848 (33%)]\tLoss: 0.138698\n",
      "Train Epoch: 94 [1192/2848 (42%)]\tLoss: 0.368817\n",
      "Train Epoch: 94 [1432/2848 (50%)]\tLoss: 0.740238\n",
      "Train Epoch: 94 [1672/2848 (59%)]\tLoss: 0.017785\n",
      "Train Epoch: 94 [1912/2848 (67%)]\tLoss: 0.486887\n",
      "Train Epoch: 94 [2152/2848 (76%)]\tLoss: 0.306006\n",
      "Train Epoch: 94 [2392/2848 (84%)]\tLoss: 0.014479\n",
      "Train Epoch: 94 [2632/2848 (92%)]\tLoss: 0.327597\n",
      "\n",
      "Test set: Average loss: 0.4484, Accuracy: 203/200 (102%)\n",
      "\n",
      "Train Epoch: 95 [232/2848 (8%)]\tLoss: 0.193356\n",
      "Train Epoch: 95 [472/2848 (17%)]\tLoss: 0.075500\n",
      "Train Epoch: 95 [712/2848 (25%)]\tLoss: 0.079044\n",
      "Train Epoch: 95 [952/2848 (33%)]\tLoss: 0.022378\n",
      "Train Epoch: 95 [1192/2848 (42%)]\tLoss: 0.962955\n",
      "Train Epoch: 95 [1432/2848 (50%)]\tLoss: 0.473089\n",
      "Train Epoch: 95 [1672/2848 (59%)]\tLoss: 0.379982\n",
      "Train Epoch: 95 [1912/2848 (67%)]\tLoss: 0.328869\n",
      "Train Epoch: 95 [2152/2848 (76%)]\tLoss: 0.258028\n",
      "Train Epoch: 95 [2392/2848 (84%)]\tLoss: 0.456436\n",
      "Train Epoch: 95 [2632/2848 (92%)]\tLoss: 0.812187\n",
      "\n",
      "Test set: Average loss: 0.4213, Accuracy: 196/200 (98%)\n",
      "\n",
      "Train Epoch: 96 [232/2848 (8%)]\tLoss: 0.070795\n",
      "Train Epoch: 96 [472/2848 (17%)]\tLoss: 0.204930\n",
      "Train Epoch: 96 [712/2848 (25%)]\tLoss: 0.029773\n",
      "Train Epoch: 96 [952/2848 (33%)]\tLoss: 0.230521\n",
      "Train Epoch: 96 [1192/2848 (42%)]\tLoss: 0.105401\n",
      "Train Epoch: 96 [1432/2848 (50%)]\tLoss: 0.074134\n",
      "Train Epoch: 96 [1672/2848 (59%)]\tLoss: 0.236325\n",
      "Train Epoch: 96 [1912/2848 (67%)]\tLoss: 0.181433\n",
      "Train Epoch: 96 [2152/2848 (76%)]\tLoss: 0.087413\n",
      "Train Epoch: 96 [2392/2848 (84%)]\tLoss: 0.246357\n",
      "Train Epoch: 96 [2632/2848 (92%)]\tLoss: 0.153575\n",
      "\n",
      "Test set: Average loss: 0.3444, Accuracy: 228/200 (114%)\n",
      "\n",
      "Train Epoch: 97 [232/2848 (8%)]\tLoss: 0.667149\n",
      "Train Epoch: 97 [472/2848 (17%)]\tLoss: 0.219713\n",
      "Train Epoch: 97 [712/2848 (25%)]\tLoss: 0.101306\n",
      "Train Epoch: 97 [952/2848 (33%)]\tLoss: 0.124951\n",
      "Train Epoch: 97 [1192/2848 (42%)]\tLoss: 0.117970\n",
      "Train Epoch: 97 [1432/2848 (50%)]\tLoss: 0.119364\n",
      "Train Epoch: 97 [1672/2848 (59%)]\tLoss: 0.082953\n",
      "Train Epoch: 97 [1912/2848 (67%)]\tLoss: 0.090661\n",
      "Train Epoch: 97 [2152/2848 (76%)]\tLoss: 0.472593\n",
      "Train Epoch: 97 [2392/2848 (84%)]\tLoss: 0.225369\n",
      "Train Epoch: 97 [2632/2848 (92%)]\tLoss: 0.629399\n",
      "\n",
      "Test set: Average loss: 0.4289, Accuracy: 214/200 (107%)\n",
      "\n",
      "Train Epoch: 98 [232/2848 (8%)]\tLoss: 0.268212\n",
      "Train Epoch: 98 [472/2848 (17%)]\tLoss: 0.116830\n",
      "Train Epoch: 98 [712/2848 (25%)]\tLoss: 0.117980\n",
      "Train Epoch: 98 [952/2848 (33%)]\tLoss: 0.039347\n",
      "Train Epoch: 98 [1192/2848 (42%)]\tLoss: 0.143005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 98 [1432/2848 (50%)]\tLoss: 0.315015\n",
      "Train Epoch: 98 [1672/2848 (59%)]\tLoss: 0.345588\n",
      "Train Epoch: 98 [1912/2848 (67%)]\tLoss: 0.326368\n",
      "Train Epoch: 98 [2152/2848 (76%)]\tLoss: 0.149024\n",
      "Train Epoch: 98 [2392/2848 (84%)]\tLoss: 0.210119\n",
      "Train Epoch: 98 [2632/2848 (92%)]\tLoss: 0.096005\n",
      "\n",
      "Test set: Average loss: 0.3439, Accuracy: 230/200 (115%)\n",
      "\n",
      "Train Epoch: 99 [232/2848 (8%)]\tLoss: 0.184578\n",
      "Train Epoch: 99 [472/2848 (17%)]\tLoss: 0.189385\n",
      "Train Epoch: 99 [712/2848 (25%)]\tLoss: 0.040343\n",
      "Train Epoch: 99 [952/2848 (33%)]\tLoss: 1.459046\n",
      "Train Epoch: 99 [1192/2848 (42%)]\tLoss: 0.348731\n",
      "Train Epoch: 99 [1432/2848 (50%)]\tLoss: 0.128946\n",
      "Train Epoch: 99 [1672/2848 (59%)]\tLoss: 0.059888\n",
      "Train Epoch: 99 [1912/2848 (67%)]\tLoss: 0.490520\n",
      "Train Epoch: 99 [2152/2848 (76%)]\tLoss: 0.207266\n",
      "Train Epoch: 99 [2392/2848 (84%)]\tLoss: 0.312900\n",
      "Train Epoch: 99 [2632/2848 (92%)]\tLoss: 0.084442\n",
      "\n",
      "Test set: Average loss: 0.3839, Accuracy: 188/200 (94%)\n",
      "\n",
      "Train Epoch: 100 [232/2848 (8%)]\tLoss: 0.613567\n",
      "Train Epoch: 100 [472/2848 (17%)]\tLoss: 0.450407\n",
      "Train Epoch: 100 [712/2848 (25%)]\tLoss: 0.363455\n",
      "Train Epoch: 100 [952/2848 (33%)]\tLoss: 0.018840\n",
      "Train Epoch: 100 [1192/2848 (42%)]\tLoss: 0.511304\n",
      "Train Epoch: 100 [1432/2848 (50%)]\tLoss: 0.176441\n",
      "Train Epoch: 100 [1672/2848 (59%)]\tLoss: 0.496569\n",
      "Train Epoch: 100 [1912/2848 (67%)]\tLoss: 0.210057\n",
      "Train Epoch: 100 [2152/2848 (76%)]\tLoss: 0.071867\n",
      "Train Epoch: 100 [2392/2848 (84%)]\tLoss: 0.097247\n",
      "Train Epoch: 100 [2632/2848 (92%)]\tLoss: 0.126600\n",
      "\n",
      "Test set: Average loss: 0.3993, Accuracy: 222/200 (111%)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-81b06110d0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrainmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtestmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'最好的识别率为：%f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "#开始训练过程\n",
    "EPOCHS = 100\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    trainmodel(model, train_, optimizer, epoch)    \n",
    "    testmodel(model,test_)\n",
    "print('最好的识别率为：%f'%(max(t_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure\n",
    "plt.plot(range(0,100),accuracy)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3], device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机读入一张照片，输出预测的结果\n",
    "\n",
    "from PIL import Image\n",
    "#读入图片\n",
    "img = Image.open('/data0/zengpz/damage/软腐病/1.jpg')  \n",
    "input=transform(img)#这里经过转换后输出的input格式是[C,H,W],网络输入还需要增加一维批量大小B\n",
    "img = torch.tensor(input).cuda()\n",
    "img = img.unsqueeze(0)#增加一维，输出的img格式为[1,C,H,W],这是因为model的输入是【batch，channel，H,W】\n",
    " \n",
    "\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 17)\n",
    "model.load_state_dict(torch.load('chonghai_params.pkl'))\n",
    "model.cuda()\n",
    " \n",
    "res = model(img)#将图片输入网络得到输出\n",
    "print(res.shape)  #torch.Size([1, 33])\n",
    "res.max(1)#按列取最大值\n",
    "res.max(1)[1] #取出最大值的下标对应的index，也就是对应的分类的label\n",
    "\n",
    "#max_value,index = t.max(probability,1)#找到最大概率对应的索引号，该图片即为该索引号对应的类别\n",
    "#print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
