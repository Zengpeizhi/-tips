{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data = pd.read_csv('train_last.csv')   # 读取数据\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "train_data = train_data.apply(preprocessing.LabelEncoder().fit_transform)#把列做label encoding\n",
    "\n",
    "#隐藏警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.grid_search import GridSearchCV  # Perforing grid search\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "cols = train_data.columns\n",
    "x, X_test, y, y_test = train_test_split(train_data[cols[0:10]], train_data['y'], test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#train_data = pd.read_csv('train_last.csv')   # 读取数据\n",
    "y = np.array(train_data['y'])  # 用pop方式将训练数据中的标签值y取出来，作为训练目标，这里的‘30’是标签的列名\n",
    "col = train_data.columns [0:10]  \n",
    "x = np.array(train_data[col].values)  # 剩下的列作为训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden_layer_1): Linear(in_features=10, out_features=48, bias=True)\n",
      "  (hidden_layer_2): Linear(in_features=48, out_features=48, bias=True)\n",
      "  (predict_layer): Linear(in_features=48, out_features=2, bias=True)\n",
      ")\n",
      "train_time:0,the loss is:3.217438\n",
      "train_time:1,the loss is:1416.449707\n",
      "train_time:2,the loss is:323.623810\n",
      "train_time:3,the loss is:30.549343\n",
      "train_time:4,the loss is:59.755661\n",
      "train_time:5,the loss is:82.863678\n",
      "train_time:6,the loss is:100.608955\n",
      "train_time:7,the loss is:113.642014\n",
      "train_time:8,the loss is:122.571793\n",
      "train_time:9,the loss is:127.531807\n",
      "train_time:10,the loss is:128.689178\n",
      "train_time:11,the loss is:126.988609\n",
      "train_time:12,the loss is:122.775017\n",
      "train_time:13,the loss is:116.362946\n",
      "train_time:14,the loss is:108.091606\n",
      "train_time:15,the loss is:98.218369\n",
      "train_time:16,the loss is:86.995026\n",
      "train_time:17,the loss is:74.675865\n",
      "train_time:18,the loss is:61.491543\n",
      "train_time:19,the loss is:47.630093\n",
      "train_time:20,the loss is:33.236942\n",
      "train_time:21,the loss is:18.409372\n",
      "train_time:22,the loss is:3.264304\n",
      "train_time:23,the loss is:372.523682\n",
      "train_time:24,the loss is:1.259242\n",
      "train_time:25,the loss is:12.432036\n",
      "train_time:26,the loss is:22.681774\n",
      "train_time:27,the loss is:31.279770\n",
      "train_time:28,the loss is:38.369209\n",
      "train_time:29,the loss is:44.077805\n",
      "train_time:30,the loss is:48.521317\n",
      "train_time:31,the loss is:51.818127\n",
      "train_time:32,the loss is:54.074467\n",
      "train_time:33,the loss is:55.398151\n",
      "train_time:34,the loss is:55.891029\n",
      "train_time:35,the loss is:55.651981\n",
      "train_time:36,the loss is:54.775066\n",
      "train_time:37,the loss is:53.359741\n",
      "train_time:38,the loss is:51.492897\n",
      "train_time:39,the loss is:49.262737\n",
      "train_time:40,the loss is:46.743385\n",
      "train_time:41,the loss is:43.996178\n",
      "train_time:42,the loss is:41.068180\n",
      "train_time:43,the loss is:38.009933\n",
      "train_time:44,the loss is:34.864491\n",
      "train_time:45,the loss is:31.678577\n",
      "train_time:46,the loss is:28.484707\n",
      "train_time:47,the loss is:25.314379\n",
      "train_time:48,the loss is:22.190248\n",
      "train_time:49,the loss is:19.131683\n",
      "train_time:50,the loss is:16.150236\n",
      "train_time:51,the loss is:13.253243\n",
      "train_time:52,the loss is:10.440271\n",
      "train_time:53,the loss is:7.706800\n",
      "train_time:54,the loss is:5.052557\n",
      "train_time:55,the loss is:2.494113\n",
      "train_time:56,the loss is:5.083515\n",
      "train_time:57,the loss is:3.319119\n",
      "train_time:58,the loss is:6.099551\n",
      "train_time:59,the loss is:8.366715\n",
      "train_time:60,the loss is:10.167666\n",
      "train_time:61,the loss is:11.546070\n",
      "train_time:62,the loss is:12.538513\n",
      "train_time:63,the loss is:13.185727\n",
      "train_time:64,the loss is:13.520269\n",
      "train_time:65,the loss is:13.553334\n",
      "train_time:66,the loss is:13.285818\n",
      "train_time:67,the loss is:12.623223\n",
      "train_time:68,the loss is:11.448199\n",
      "train_time:69,the loss is:9.898435\n",
      "train_time:70,the loss is:8.118660\n",
      "train_time:71,the loss is:6.122463\n",
      "train_time:72,the loss is:4.083945\n",
      "train_time:73,the loss is:2.619610\n",
      "train_time:74,the loss is:1.727544\n",
      "train_time:75,the loss is:1.270342\n",
      "train_time:76,the loss is:1.230165\n",
      "train_time:77,the loss is:2.470946\n",
      "train_time:78,the loss is:3.981910\n",
      "train_time:79,the loss is:4.960068\n",
      "train_time:80,the loss is:5.411341\n",
      "train_time:81,the loss is:5.370497\n",
      "train_time:82,the loss is:4.893971\n",
      "train_time:83,the loss is:4.046845\n",
      "train_time:84,the loss is:2.895247\n",
      "train_time:85,the loss is:1.503125\n",
      "train_time:86,the loss is:20.743820\n",
      "train_time:87,the loss is:4.558993\n",
      "train_time:88,the loss is:8.561790\n",
      "train_time:89,the loss is:11.956834\n",
      "train_time:90,the loss is:14.773084\n",
      "train_time:91,the loss is:17.041115\n",
      "train_time:92,the loss is:18.796030\n",
      "train_time:93,the loss is:20.075541\n",
      "train_time:94,the loss is:20.918293\n",
      "train_time:95,the loss is:21.367434\n",
      "train_time:96,the loss is:21.464828\n",
      "train_time:97,the loss is:21.252655\n",
      "train_time:98,the loss is:20.771784\n",
      "train_time:99,the loss is:20.063639\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 自定义一个Net类，继承于torch.nn.Module类\n",
    "# 这个神经网络的设计是只有一层隐含层，隐含层神经元个数可随意指定\n",
    "class Net(torch.nn.Module):\n",
    "    # Net类的初始化函数\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        # 继承父类的初始化函数\n",
    "        super(Net, self).__init__()\n",
    "        # 网络的隐藏层创建，名称可以随便起\n",
    "        self.hidden_layer_1 = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.hidden_layer_2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        # 输出层(预测层)创建，接收来自隐含层的数据\n",
    "        self.predict_layer = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    # 网络的前向传播函数，构造计算图\n",
    "    def forward(self, x):\n",
    "        # 用relu函数处理隐含层输出的结果并传给输出层\n",
    "        hidden_result_1 = self.hidden_layer_1(x)\n",
    "        relu_result_1 = F.relu(hidden_result_1)\n",
    "        \n",
    "        hidden_result_2 = self.hidden_layer_2(relu_result_1)\n",
    "        relu_result_2 = F.relu(hidden_result_2)\n",
    "        \n",
    "        predict_result = self.predict_layer(relu_result_2)\n",
    "        return predict_result\n",
    "\n",
    "\n",
    "# 训练次数\n",
    "TRAIN_TIMES = 100\n",
    "# 输入输出的数据维度，这里都是1维\n",
    "INPUT_FEATURE_DIM = 10\n",
    "OUTPUT_FEATURE_DIM = 2\n",
    "# 隐含层中神经元的个数\n",
    "NEURON_NUM = 48\n",
    "# 学习率，越大学的越快，但也容易造成不稳定，准确率上下波动的情况\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# 数据构造\n",
    "# 这里x_data、y_data都是tensor格式，在PyTorch0.4版本以后，也能进行反向传播\n",
    "# 所以不需要再转成Variable格式了\n",
    "# linspace函数用于生成一系列数据\n",
    "# unsqueeze函数可以将一维数据变成二维数据，在torch中只能处理二维数据\n",
    "#x_data = torch.unsqueeze(torch.linspace(-4, 4, 80), dim=1)\n",
    "# randn函数用于生成服从正态分布的随机数\n",
    "\n",
    "x = torch.from_numpy(x)\n",
    "#x = torch.tensor(x,dtype=torch.long)\n",
    "#x_data = torch.unsqueeze(x,dim=1)\n",
    "#y_data = torch.tensor(y_data,dtype=torch.long)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "# 建立网络\n",
    "net = Net(n_feature=INPUT_FEATURE_DIM, n_hidden=NEURON_NUM, n_output=OUTPUT_FEATURE_DIM)\n",
    "print(net)\n",
    "\n",
    "\n",
    "# 训练网络\n",
    "# 这里也可以使用其它的优化方法\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "# 定义一个误差计算方法\n",
    "loss_func =  torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(TRAIN_TIMES):\n",
    "    # 输入数据进行预测\n",
    "    prediction = net(x.float())\n",
    "    # 计算预测值与真值误差，注意参数顺序问题\n",
    "    # 第一个参数为预测值，第二个为真值\n",
    "    loss = loss_func(prediction, y)\n",
    "\n",
    "    # 开始优化步骤\n",
    "    # 每次开始优化前将梯度置为0\n",
    "    optimizer.zero_grad()\n",
    "    # 误差反向传播\n",
    "    loss.backward()\n",
    "    # 按照最小loss优化参数\n",
    "    optimizer.step()\n",
    "    print('train_time:{:.0f},the loss is:{:.6f}'.format(i,loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict,'pre_nn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "pre = net(X_test.float())\n",
    "pre_label = pre.max(1)[1]  #输出预测概率最大的值对应的下标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9681174127322478"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = torch.tensor(np.array(y_test))\n",
    "correct_pre = (pre_label == y_test).sum()\n",
    "accuracy = correct_pre.numpy()/len(y_test)\n",
    "accuracy  #输出准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.size>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = pre.softmax(1)\n",
    "y_score = y_score.detach().numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画出auc曲线\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "score = []\n",
    "j = 0 \n",
    "for i in y_test:\n",
    "    score.append(y_score[j,i])\n",
    "    j += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr=[]\n",
    "tpr = []\n",
    "fpr, tpr, _ = roc_curve(y_test, score)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#画出AUC曲线图\n",
    "plt.plot(fpr,tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.76105e-40,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.91479e-40,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
